{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/columba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>commitdate</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2001/12/12 17:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.579380</td>\n",
       "      <td>0.093620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>480.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>596</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>143</td>\n",
       "      <td>133.50</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1999/10/12 12:57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>140</td>\n",
       "      <td>140.00</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2002/5/15 16:55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>0.739279</td>\n",
       "      <td>0.183477</td>\n",
       "      <td>0.208913</td>\n",
       "      <td>283.519231</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>15836</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>984</td>\n",
       "      <td>818.65</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2002/1/21 15:37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.685328</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>514.375000</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>579</td>\n",
       "      <td>479.25</td>\n",
       "      <td>550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2001/12/19 16:44</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>0.769776</td>\n",
       "      <td>0.091829</td>\n",
       "      <td>0.072746</td>\n",
       "      <td>366.815789</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>6565</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>413</td>\n",
       "      <td>313.25</td>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transactionid        commitdate  ns  nm  nf   entropy        la        ld  \\\n",
       "0              3  2001/12/12 17:41   1   1   3  0.579380  0.093620  0.000000   \n",
       "1              7  1999/10/12 12:57   1   1   1  0.000000  0.000000  0.000000   \n",
       "2              8   2002/5/15 16:55   3   3  52  0.739279  0.183477  0.208913   \n",
       "3              9   2002/1/21 15:37   1   1   8  0.685328  0.016039  0.012880   \n",
       "4             10  2001/12/19 16:44   2   2  38  0.769776  0.091829  0.072746   \n",
       "\n",
       "           lt  fix  ndev     pd       npt  exp    rexp  sexp  bug  \n",
       "0  480.666667    1    14    596  0.666667  143  133.50   129    1  \n",
       "1  398.000000    1     1      0  1.000000  140  140.00   137    1  \n",
       "2  283.519231    0    23  15836  0.750000  984  818.65   978    0  \n",
       "3  514.375000    1    21   1281  1.000000  579  479.25   550    0  \n",
       "4  366.815789    1    21   6565  0.763158  413  313.25   405    0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X.drop(\"commitdate\", axis =1, inplace=True)\n",
    "y = df.iloc[:, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3234, 15)\n",
      "(1386, 15)\n",
      "(3234,)\n",
      "(1386,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ロジスティック回帰の混合行列 [[785  90]\n",
      " [322 189]]\n",
      "ロジスティック回帰での正答率 0.7027417027417028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_p_lr = lr.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_lr))\n",
    "\n",
    "print (\"ロジスティック回帰での正答率\", accuracy_score(y_test, y_p_lr))\n",
    "#ロジスティック回帰正答率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# svm = SVC(kernel='linear')\n",
    "\n",
    "# svm.fit(X_train, y_train)\n",
    "# y_p_svm = svm.predict(X_test)\n",
    "# # 正答率を算出\n",
    "# print('SVMでの正答率', accuracy_score(y_test, y_p_svm))\n",
    "# print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knnの混合行列 [[777  98]\n",
      " [377 134]]\n",
      "knnでの正答率 0.6572871572871573\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "list_nn = []\n",
    "list_score = []\n",
    "for k in range(1, 5):\n",
    "    knc = KNeighborsClassifier(n_neighbors=k)\n",
    "    knc.fit(X_train, y_train)\n",
    "    Y_pred = knc.predict(X_test)\n",
    "    score = knc.score(X_test, y_test)\n",
    "#     print(\"[%d] score: {:.2f}\".format(score) % k)\n",
    "    list_nn.append(k)\n",
    "    list_score.append(score)\n",
    "\n",
    "#プロット\n",
    "plt.ylim(0.1, 1.0)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(list_nn, list_score)\n",
    "\n",
    "#k = 2の時が良い\n",
    "knc = KNeighborsClassifier(n_neighbors=2)\n",
    "knc.fit(X_train, y_train)\n",
    "Y_pred = knc.predict(X_test)\n",
    "score = knc.score(X_test, y_test)\n",
    "print(\"knnの混合行列\", confusion_matrix(y_test, Y_pred))\n",
    "print (\"knnでの正答率\", accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.convolutional import Conv1D, UpSampling1D\n",
    "from keras.layers.pooling import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Add, Input\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# lookback = 5\n",
    "\n",
    "# #データを4次元化\n",
    "# X_train = X_train.reshape((len(X_train),lookback,1,1))\n",
    "# X_val = X_val.reshape((len(X_val),lookback,1,1))\n",
    "# X_test = X_test.reshape((len(X_test),lookback,1,1))\n",
    "\n",
    "# #CNNの学習\n",
    "# input_ = Input(shape=(lookback, 1,1))#横の数、縦の数、RGB\n",
    "\n",
    "# c = Conv2D(8, (3, 1),padding='same',activation='relu')(input_)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = MaxPooling2D(pool_size=(2, 1))(c)\n",
    "\n",
    "# c = Flatten()(c)\n",
    "# c = Dense(30,activation='relu')(c)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = Dense(4, activation='softmax')(c)\n",
    "\n",
    "# model = Model(input_, c)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# hist = model.fit(X_train, Y_train, batch_size = 10, epochs=100, verbose=1, shuffle=True,\n",
    "#                  validation_data = (X_val,Y_val))\n",
    "# #結果描画\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_loss'],label=\"val_loss\")\n",
    "# plt.plot(hist.history['loss'],label=\"train_loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_acc'],label=\"val_acc\")\n",
    "# plt.plot(hist.history['acc'],label=\"train_acc\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3234 samples, validate on 1386 samples\n",
      "Epoch 1/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6568 - acc: 0.6197 - val_loss: 0.6489 - val_acc: 0.6313\n",
      "Epoch 2/200\n",
      "3234/3234 [==============================] - 3s 1ms/step - loss: 0.6457 - acc: 0.6274 - val_loss: 0.6698 - val_acc: 0.6082\n",
      "Epoch 3/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6349 - acc: 0.6327 - val_loss: 0.6818 - val_acc: 0.6284\n",
      "Epoch 4/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6410 - acc: 0.6262 - val_loss: 0.6574 - val_acc: 0.6126\n",
      "Epoch 5/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6327 - acc: 0.6296 - val_loss: 0.6822 - val_acc: 0.5317\n",
      "Epoch 6/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6310 - acc: 0.6361 - val_loss: 0.6487 - val_acc: 0.6356\n",
      "Epoch 7/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6295 - acc: 0.6314 - val_loss: 0.6501 - val_acc: 0.6133\n",
      "Epoch 8/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6243 - acc: 0.6404 - val_loss: 0.6457 - val_acc: 0.6349\n",
      "Epoch 9/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6265 - acc: 0.6447 - val_loss: 0.6462 - val_acc: 0.6169\n",
      "Epoch 10/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6237 - acc: 0.6373 - val_loss: 0.6442 - val_acc: 0.6147\n",
      "Epoch 11/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6173 - acc: 0.6509 - val_loss: 0.6394 - val_acc: 0.6205\n",
      "Epoch 12/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6223 - acc: 0.6429 - val_loss: 0.6787 - val_acc: 0.5519\n",
      "Epoch 13/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6245 - acc: 0.6463 - val_loss: 0.6640 - val_acc: 0.5707\n",
      "Epoch 14/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6168 - acc: 0.6509 - val_loss: 0.6965 - val_acc: 0.6306\n",
      "Epoch 15/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6125 - acc: 0.6565 - val_loss: 0.6564 - val_acc: 0.5859\n",
      "Epoch 16/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6183 - acc: 0.6528 - val_loss: 0.6707 - val_acc: 0.5628\n",
      "Epoch 17/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6100 - acc: 0.6515 - val_loss: 0.6396 - val_acc: 0.6457\n",
      "Epoch 18/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6120 - acc: 0.6466 - val_loss: 0.6533 - val_acc: 0.6104\n",
      "Epoch 19/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6097 - acc: 0.6580 - val_loss: 0.6435 - val_acc: 0.6392\n",
      "Epoch 20/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6123 - acc: 0.6617 - val_loss: 0.6479 - val_acc: 0.5960\n",
      "Epoch 21/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6109 - acc: 0.6580 - val_loss: 0.6686 - val_acc: 0.6097\n",
      "Epoch 22/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6141 - acc: 0.6549 - val_loss: 0.6456 - val_acc: 0.6169\n",
      "Epoch 23/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6101 - acc: 0.6565 - val_loss: 0.7475 - val_acc: 0.5065\n",
      "Epoch 24/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6140 - acc: 0.6478 - val_loss: 0.6542 - val_acc: 0.5779\n",
      "Epoch 25/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6066 - acc: 0.6543 - val_loss: 0.6459 - val_acc: 0.6003\n",
      "Epoch 26/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6079 - acc: 0.6503 - val_loss: 0.6685 - val_acc: 0.6364\n",
      "Epoch 27/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6065 - acc: 0.6617 - val_loss: 0.6386 - val_acc: 0.6472\n",
      "Epoch 28/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6068 - acc: 0.6636 - val_loss: 0.6837 - val_acc: 0.6010\n",
      "Epoch 29/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6091 - acc: 0.6515 - val_loss: 0.6353 - val_acc: 0.6407\n",
      "Epoch 30/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6039 - acc: 0.6596 - val_loss: 0.6902 - val_acc: 0.6313\n",
      "Epoch 31/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6038 - acc: 0.6614 - val_loss: 0.6320 - val_acc: 0.6392\n",
      "Epoch 32/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5976 - acc: 0.6673 - val_loss: 0.6610 - val_acc: 0.5801\n",
      "Epoch 33/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6052 - acc: 0.6558 - val_loss: 0.6814 - val_acc: 0.6407\n",
      "Epoch 34/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6050 - acc: 0.6512 - val_loss: 0.6328 - val_acc: 0.6349\n",
      "Epoch 35/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6020 - acc: 0.6592 - val_loss: 0.6321 - val_acc: 0.6421\n",
      "Epoch 36/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6008 - acc: 0.6654 - val_loss: 0.7784 - val_acc: 0.4986\n",
      "Epoch 37/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6011 - acc: 0.6583 - val_loss: 0.6434 - val_acc: 0.6017\n",
      "Epoch 38/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5979 - acc: 0.6592 - val_loss: 0.6977 - val_acc: 0.5332\n",
      "Epoch 39/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5975 - acc: 0.6645 - val_loss: 0.6830 - val_acc: 0.5613\n",
      "Epoch 40/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5980 - acc: 0.6654 - val_loss: 0.6334 - val_acc: 0.6602\n",
      "Epoch 41/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.6016 - acc: 0.6667 - val_loss: 0.6467 - val_acc: 0.6443\n",
      "Epoch 42/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5938 - acc: 0.6753 - val_loss: 0.7047 - val_acc: 0.6378\n",
      "Epoch 43/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5960 - acc: 0.6673 - val_loss: 0.6243 - val_acc: 0.6494\n",
      "Epoch 44/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5948 - acc: 0.6756 - val_loss: 0.6685 - val_acc: 0.5902\n",
      "Epoch 45/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5918 - acc: 0.6614 - val_loss: 0.6665 - val_acc: 0.5750\n",
      "Epoch 46/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5929 - acc: 0.6725 - val_loss: 0.6819 - val_acc: 0.5491\n",
      "Epoch 47/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5972 - acc: 0.6645 - val_loss: 0.6235 - val_acc: 0.6479\n",
      "Epoch 48/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5907 - acc: 0.6701 - val_loss: 0.6526 - val_acc: 0.6154\n",
      "Epoch 49/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5927 - acc: 0.6633 - val_loss: 0.6442 - val_acc: 0.6025\n",
      "Epoch 50/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5868 - acc: 0.6756 - val_loss: 0.6349 - val_acc: 0.6219\n",
      "Epoch 51/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5891 - acc: 0.6763 - val_loss: 0.7163 - val_acc: 0.5245\n",
      "Epoch 52/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5839 - acc: 0.6793 - val_loss: 0.6704 - val_acc: 0.5599\n",
      "Epoch 53/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5931 - acc: 0.6784 - val_loss: 0.6394 - val_acc: 0.6140\n",
      "Epoch 54/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5886 - acc: 0.6679 - val_loss: 0.6343 - val_acc: 0.6537\n",
      "Epoch 55/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5878 - acc: 0.6756 - val_loss: 0.6295 - val_acc: 0.6645\n",
      "Epoch 56/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5883 - acc: 0.6772 - val_loss: 0.6484 - val_acc: 0.6190\n",
      "Epoch 57/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5874 - acc: 0.6747 - val_loss: 0.7233 - val_acc: 0.5159\n",
      "Epoch 58/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5890 - acc: 0.6797 - val_loss: 0.6334 - val_acc: 0.6652\n",
      "Epoch 59/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5831 - acc: 0.6784 - val_loss: 0.8128 - val_acc: 0.4798\n",
      "Epoch 60/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5849 - acc: 0.6753 - val_loss: 0.6453 - val_acc: 0.6537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5862 - acc: 0.6735 - val_loss: 0.8231 - val_acc: 0.4841\n",
      "Epoch 62/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5821 - acc: 0.6781 - val_loss: 0.6439 - val_acc: 0.5823\n",
      "Epoch 63/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5793 - acc: 0.6793 - val_loss: 0.6554 - val_acc: 0.6522\n",
      "Epoch 64/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5859 - acc: 0.6800 - val_loss: 0.6118 - val_acc: 0.6696\n",
      "Epoch 65/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5783 - acc: 0.6824 - val_loss: 0.6255 - val_acc: 0.6320\n",
      "Epoch 66/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5822 - acc: 0.6824 - val_loss: 0.6658 - val_acc: 0.5642\n",
      "Epoch 67/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5813 - acc: 0.6812 - val_loss: 0.6223 - val_acc: 0.6587\n",
      "Epoch 68/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5778 - acc: 0.6855 - val_loss: 0.7173 - val_acc: 0.5404\n",
      "Epoch 69/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5769 - acc: 0.6849 - val_loss: 0.8452 - val_acc: 0.4841\n",
      "Epoch 70/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5815 - acc: 0.6797 - val_loss: 0.6210 - val_acc: 0.6652\n",
      "Epoch 71/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5783 - acc: 0.6812 - val_loss: 0.6221 - val_acc: 0.6616\n",
      "Epoch 72/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5787 - acc: 0.6716 - val_loss: 0.7232 - val_acc: 0.5317\n",
      "Epoch 73/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5776 - acc: 0.6834 - val_loss: 0.7272 - val_acc: 0.5476\n",
      "Epoch 74/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5718 - acc: 0.6790 - val_loss: 0.7410 - val_acc: 0.5036\n",
      "Epoch 75/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5774 - acc: 0.6926 - val_loss: 0.7902 - val_acc: 0.5108\n",
      "Epoch 76/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5781 - acc: 0.6759 - val_loss: 0.6190 - val_acc: 0.6609\n",
      "Epoch 77/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5741 - acc: 0.6846 - val_loss: 0.6456 - val_acc: 0.6068\n",
      "Epoch 78/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5732 - acc: 0.6917 - val_loss: 0.6589 - val_acc: 0.6515\n",
      "Epoch 79/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5732 - acc: 0.6855 - val_loss: 0.6779 - val_acc: 0.5584\n",
      "Epoch 80/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5685 - acc: 0.6855 - val_loss: 0.7547 - val_acc: 0.5368\n",
      "Epoch 81/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5797 - acc: 0.6809 - val_loss: 0.6438 - val_acc: 0.6595\n",
      "Epoch 82/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5696 - acc: 0.6880 - val_loss: 0.6221 - val_acc: 0.6530\n",
      "Epoch 83/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5706 - acc: 0.6877 - val_loss: 0.6316 - val_acc: 0.6681\n",
      "Epoch 84/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5678 - acc: 0.6852 - val_loss: 0.6170 - val_acc: 0.6602\n",
      "Epoch 85/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5737 - acc: 0.6840 - val_loss: 0.6324 - val_acc: 0.6291\n",
      "Epoch 86/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5667 - acc: 0.6911 - val_loss: 0.7500 - val_acc: 0.5411\n",
      "Epoch 87/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5720 - acc: 0.6877 - val_loss: 0.6407 - val_acc: 0.6479\n",
      "Epoch 88/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5653 - acc: 0.6939 - val_loss: 0.7544 - val_acc: 0.5440\n",
      "Epoch 89/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5644 - acc: 0.6908 - val_loss: 0.9477 - val_acc: 0.4603\n",
      "Epoch 90/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5678 - acc: 0.6892 - val_loss: 0.6227 - val_acc: 0.6825\n",
      "Epoch 91/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5694 - acc: 0.6877 - val_loss: 0.9725 - val_acc: 0.4668\n",
      "Epoch 92/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5637 - acc: 0.6880 - val_loss: 0.6425 - val_acc: 0.5924\n",
      "Epoch 93/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5641 - acc: 0.6889 - val_loss: 0.7298 - val_acc: 0.5296\n",
      "Epoch 94/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5648 - acc: 0.6892 - val_loss: 0.6192 - val_acc: 0.6414\n",
      "Epoch 95/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5668 - acc: 0.6855 - val_loss: 0.7029 - val_acc: 0.5722\n",
      "Epoch 96/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5609 - acc: 0.6908 - val_loss: 0.6995 - val_acc: 0.5584\n",
      "Epoch 97/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5696 - acc: 0.6800 - val_loss: 0.6276 - val_acc: 0.6623\n",
      "Epoch 98/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5640 - acc: 0.6868 - val_loss: 0.6379 - val_acc: 0.6053\n",
      "Epoch 99/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5641 - acc: 0.6945 - val_loss: 0.6237 - val_acc: 0.6659\n",
      "Epoch 100/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5561 - acc: 0.7016 - val_loss: 0.7749 - val_acc: 0.5036\n",
      "Epoch 101/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5663 - acc: 0.6874 - val_loss: 0.7734 - val_acc: 0.5245\n",
      "Epoch 102/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5602 - acc: 0.6886 - val_loss: 0.9004 - val_acc: 0.4675\n",
      "Epoch 103/200\n",
      "3234/3234 [==============================] - 5s 2ms/step - loss: 0.5655 - acc: 0.6781 - val_loss: 0.6379 - val_acc: 0.6631\n",
      "Epoch 104/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5607 - acc: 0.6951 - val_loss: 0.7118 - val_acc: 0.5418\n",
      "Epoch 105/200\n",
      "3234/3234 [==============================] - 5s 2ms/step - loss: 0.5574 - acc: 0.6942 - val_loss: 0.7241 - val_acc: 0.5339\n",
      "Epoch 106/200\n",
      "3234/3234 [==============================] - 5s 2ms/step - loss: 0.5618 - acc: 0.6874 - val_loss: 0.6988 - val_acc: 0.5657\n",
      "Epoch 107/200\n",
      "3234/3234 [==============================] - 3s 1ms/step - loss: 0.5607 - acc: 0.6899 - val_loss: 0.6307 - val_acc: 0.6580\n",
      "Epoch 108/200\n",
      "3234/3234 [==============================] - 3s 1ms/step - loss: 0.5589 - acc: 0.6880 - val_loss: 0.8177 - val_acc: 0.5072\n",
      "Epoch 109/200\n",
      "3234/3234 [==============================] - 3s 1ms/step - loss: 0.5608 - acc: 0.6985 - val_loss: 0.6302 - val_acc: 0.6739\n",
      "Epoch 110/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5551 - acc: 0.7004 - val_loss: 0.6221 - val_acc: 0.6544\n",
      "Epoch 111/200\n",
      "3234/3234 [==============================] - 6s 2ms/step - loss: 0.5554 - acc: 0.7004 - val_loss: 0.7331 - val_acc: 0.5462\n",
      "Epoch 112/200\n",
      "3234/3234 [==============================] - 7s 2ms/step - loss: 0.5592 - acc: 0.6923 - val_loss: 0.6734 - val_acc: 0.5823\n",
      "Epoch 113/200\n",
      "3234/3234 [==============================] - 6s 2ms/step - loss: 0.5539 - acc: 0.6942 - val_loss: 0.6442 - val_acc: 0.6551\n",
      "Epoch 114/200\n",
      "3234/3234 [==============================] - 6s 2ms/step - loss: 0.5569 - acc: 0.6960 - val_loss: 0.6391 - val_acc: 0.6277\n",
      "Epoch 115/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5561 - acc: 0.6994 - val_loss: 0.7611 - val_acc: 0.5260\n",
      "Epoch 116/200\n",
      "3234/3234 [==============================] - 6s 2ms/step - loss: 0.5538 - acc: 0.6957 - val_loss: 0.6757 - val_acc: 0.5988\n",
      "Epoch 117/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5557 - acc: 0.6895 - val_loss: 0.6359 - val_acc: 0.6198\n",
      "Epoch 118/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5562 - acc: 0.6991 - val_loss: 0.7946 - val_acc: 0.5101\n",
      "Epoch 119/200\n",
      "3234/3234 [==============================] - 5s 2ms/step - loss: 0.5559 - acc: 0.6954 - val_loss: 0.8514 - val_acc: 0.4683\n",
      "Epoch 120/200\n",
      "3234/3234 [==============================] - 5s 2ms/step - loss: 0.5588 - acc: 0.6933 - val_loss: 0.6440 - val_acc: 0.6082\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5574 - acc: 0.6871 - val_loss: 0.7664 - val_acc: 0.5216\n",
      "Epoch 122/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5525 - acc: 0.6991 - val_loss: 0.6833 - val_acc: 0.5613\n",
      "Epoch 123/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5519 - acc: 0.7013 - val_loss: 0.8395 - val_acc: 0.4986\n",
      "Epoch 124/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5543 - acc: 0.6945 - val_loss: 0.7522 - val_acc: 0.5296\n",
      "Epoch 125/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5530 - acc: 0.7028 - val_loss: 0.9916 - val_acc: 0.4343\n",
      "Epoch 126/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5520 - acc: 0.6917 - val_loss: 0.6350 - val_acc: 0.6429\n",
      "Epoch 127/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5476 - acc: 0.7056 - val_loss: 0.7688 - val_acc: 0.5224\n",
      "Epoch 128/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5456 - acc: 0.7044 - val_loss: 0.6259 - val_acc: 0.6371\n",
      "Epoch 129/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5490 - acc: 0.7010 - val_loss: 0.7536 - val_acc: 0.5390\n",
      "Epoch 130/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5504 - acc: 0.6973 - val_loss: 0.6307 - val_acc: 0.6248\n",
      "Epoch 131/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5472 - acc: 0.7047 - val_loss: 0.6946 - val_acc: 0.5693\n",
      "Epoch 132/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5482 - acc: 0.7047 - val_loss: 0.6449 - val_acc: 0.6140\n",
      "Epoch 133/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5504 - acc: 0.7062 - val_loss: 0.7499 - val_acc: 0.5339\n",
      "Epoch 134/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5496 - acc: 0.7013 - val_loss: 0.6300 - val_acc: 0.6407\n",
      "Epoch 135/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5510 - acc: 0.6883 - val_loss: 0.6213 - val_acc: 0.6580\n",
      "Epoch 136/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5526 - acc: 0.7013 - val_loss: 0.6538 - val_acc: 0.5960\n",
      "Epoch 137/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5432 - acc: 0.7062 - val_loss: 0.7101 - val_acc: 0.5469\n",
      "Epoch 138/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5415 - acc: 0.7028 - val_loss: 0.6190 - val_acc: 0.6320\n",
      "Epoch 139/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5412 - acc: 0.7044 - val_loss: 0.8796 - val_acc: 0.5079\n",
      "Epoch 140/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5451 - acc: 0.7038 - val_loss: 0.6706 - val_acc: 0.5974\n",
      "Epoch 141/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5447 - acc: 0.7022 - val_loss: 0.6408 - val_acc: 0.6335\n",
      "Epoch 142/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5488 - acc: 0.6998 - val_loss: 0.6148 - val_acc: 0.6631\n",
      "Epoch 143/200\n",
      "3234/3234 [==============================] - 3s 1ms/step - loss: 0.5418 - acc: 0.7016 - val_loss: 0.6345 - val_acc: 0.6566\n",
      "Epoch 144/200\n",
      "3234/3234 [==============================] - 3s 1ms/step - loss: 0.5400 - acc: 0.7066 - val_loss: 0.6440 - val_acc: 0.6053\n",
      "Epoch 145/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5421 - acc: 0.7022 - val_loss: 0.6426 - val_acc: 0.6609\n",
      "Epoch 146/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5469 - acc: 0.6964 - val_loss: 0.8570 - val_acc: 0.5000\n",
      "Epoch 147/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5342 - acc: 0.7084 - val_loss: 0.6158 - val_acc: 0.6602\n",
      "Epoch 148/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5456 - acc: 0.7022 - val_loss: 0.6337 - val_acc: 0.6631\n",
      "Epoch 149/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5441 - acc: 0.7081 - val_loss: 0.6991 - val_acc: 0.5786\n",
      "Epoch 150/200\n",
      "3234/3234 [==============================] - 6s 2ms/step - loss: 0.5423 - acc: 0.7025 - val_loss: 0.7193 - val_acc: 0.5620\n",
      "Epoch 151/200\n",
      "3234/3234 [==============================] - 6s 2ms/step - loss: 0.5413 - acc: 0.7081 - val_loss: 0.6097 - val_acc: 0.6616\n",
      "Epoch 152/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5408 - acc: 0.7066 - val_loss: 0.6725 - val_acc: 0.5837\n",
      "Epoch 153/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5448 - acc: 0.7028 - val_loss: 0.8118 - val_acc: 0.5303\n",
      "Epoch 154/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5350 - acc: 0.7050 - val_loss: 0.6948 - val_acc: 0.5714\n",
      "Epoch 155/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5367 - acc: 0.7137 - val_loss: 0.6783 - val_acc: 0.5952\n",
      "Epoch 156/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5436 - acc: 0.7035 - val_loss: 0.6881 - val_acc: 0.5722\n",
      "Epoch 157/200\n",
      "3234/3234 [==============================] - 5s 2ms/step - loss: 0.5412 - acc: 0.7047 - val_loss: 0.8673 - val_acc: 0.5065\n",
      "Epoch 158/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5334 - acc: 0.7189 - val_loss: 1.3522 - val_acc: 0.4293\n",
      "Epoch 159/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5403 - acc: 0.6976 - val_loss: 0.8634 - val_acc: 0.4841\n",
      "Epoch 160/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5334 - acc: 0.7155 - val_loss: 0.6702 - val_acc: 0.6032\n",
      "Epoch 161/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5321 - acc: 0.7149 - val_loss: 0.6283 - val_acc: 0.6508\n",
      "Epoch 162/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5324 - acc: 0.7223 - val_loss: 0.7363 - val_acc: 0.5469\n",
      "Epoch 163/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5358 - acc: 0.7022 - val_loss: 0.6649 - val_acc: 0.5931\n",
      "Epoch 164/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5381 - acc: 0.7140 - val_loss: 0.9422 - val_acc: 0.5007\n",
      "Epoch 165/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5306 - acc: 0.7195 - val_loss: 0.6363 - val_acc: 0.6147\n",
      "Epoch 166/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5349 - acc: 0.7081 - val_loss: 0.6467 - val_acc: 0.6162\n",
      "Epoch 167/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5283 - acc: 0.7158 - val_loss: 0.6458 - val_acc: 0.6255\n",
      "Epoch 168/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5365 - acc: 0.7050 - val_loss: 0.7028 - val_acc: 0.5534\n",
      "Epoch 169/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5297 - acc: 0.7152 - val_loss: 0.7368 - val_acc: 0.5556\n",
      "Epoch 170/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5349 - acc: 0.7041 - val_loss: 0.6293 - val_acc: 0.6328\n",
      "Epoch 171/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5296 - acc: 0.7168 - val_loss: 1.0846 - val_acc: 0.4690\n",
      "Epoch 172/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5303 - acc: 0.7158 - val_loss: 0.6769 - val_acc: 0.5844\n",
      "Epoch 173/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5324 - acc: 0.7109 - val_loss: 0.6886 - val_acc: 0.6010\n",
      "Epoch 174/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5310 - acc: 0.7137 - val_loss: 0.7971 - val_acc: 0.5462\n",
      "Epoch 175/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5318 - acc: 0.7069 - val_loss: 0.6211 - val_acc: 0.6595\n",
      "Epoch 176/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5353 - acc: 0.7090 - val_loss: 0.6189 - val_acc: 0.6623\n",
      "Epoch 177/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5298 - acc: 0.7149 - val_loss: 0.9087 - val_acc: 0.4798\n",
      "Epoch 178/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5267 - acc: 0.7161 - val_loss: 0.6709 - val_acc: 0.5873\n",
      "Epoch 179/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5287 - acc: 0.7072 - val_loss: 0.7291 - val_acc: 0.5671\n",
      "Epoch 180/200\n",
      "3234/3234 [==============================] - 5s 1ms/step - loss: 0.5266 - acc: 0.7143 - val_loss: 0.7528 - val_acc: 0.5592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5207 - acc: 0.7251 - val_loss: 0.6467 - val_acc: 0.6356\n",
      "Epoch 182/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5248 - acc: 0.7106 - val_loss: 0.6625 - val_acc: 0.5988\n",
      "Epoch 183/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5240 - acc: 0.7140 - val_loss: 0.6491 - val_acc: 0.6631\n",
      "Epoch 184/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5245 - acc: 0.7161 - val_loss: 0.6471 - val_acc: 0.6299\n",
      "Epoch 185/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5298 - acc: 0.7106 - val_loss: 0.7318 - val_acc: 0.5678\n",
      "Epoch 186/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5262 - acc: 0.7152 - val_loss: 0.7906 - val_acc: 0.5354\n",
      "Epoch 187/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5268 - acc: 0.7100 - val_loss: 0.6088 - val_acc: 0.6472\n",
      "Epoch 188/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5281 - acc: 0.7109 - val_loss: 0.7173 - val_acc: 0.5837\n",
      "Epoch 189/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5194 - acc: 0.7174 - val_loss: 1.0011 - val_acc: 0.5051\n",
      "Epoch 190/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5193 - acc: 0.7263 - val_loss: 0.6368 - val_acc: 0.6623\n",
      "Epoch 191/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5262 - acc: 0.7118 - val_loss: 0.7356 - val_acc: 0.5722\n",
      "Epoch 192/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5208 - acc: 0.7217 - val_loss: 0.6859 - val_acc: 0.5859\n",
      "Epoch 193/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5276 - acc: 0.7161 - val_loss: 1.2037 - val_acc: 0.4242\n",
      "Epoch 194/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5206 - acc: 0.7208 - val_loss: 0.6250 - val_acc: 0.6566\n",
      "Epoch 195/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5227 - acc: 0.7233 - val_loss: 0.7570 - val_acc: 0.5664\n",
      "Epoch 196/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5217 - acc: 0.7205 - val_loss: 0.9362 - val_acc: 0.4812\n",
      "Epoch 197/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5212 - acc: 0.7183 - val_loss: 0.6412 - val_acc: 0.6248\n",
      "Epoch 198/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5158 - acc: 0.7267 - val_loss: 0.6635 - val_acc: 0.6234\n",
      "Epoch 199/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5234 - acc: 0.7267 - val_loss: 0.7395 - val_acc: 0.5332\n",
      "Epoch 200/200\n",
      "3234/3234 [==============================] - 4s 1ms/step - loss: 0.5195 - acc: 0.7211 - val_loss: 0.9781 - val_acc: 0.4747\n",
      "0.5157555308176841 0.6233766232906138\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import keras \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "      \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  \n",
    "nb_epochs = 200\n",
    "\n",
    "\n",
    "#flist = ['Adiac', 'Beef', 'CBF', 'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', \n",
    "#'DiatomSizeReduction', 'ECGFiveDays', 'FaceAll', 'FaceFour', 'FacesUCR', '50words', 'FISH', 'Gun_Point', 'Haptics', \n",
    "#'InlineSkate', 'ItalyPowerDemand', 'Lighting2', 'Lighting7', 'MALLAT', 'MedicalImages', 'MoteStrain', 'NonInvasiveFatalECG_Thorax1', \n",
    "#'NonInvasiveFatalECG_Thorax2', 'OliveOil', 'OSULeaf', 'SonyAIBORobotSurface', 'SonyAIBORobotSurfaceII', 'StarLightCurves', 'SwedishLeaf', 'Symbols', \n",
    "#'synthetic_control', 'Trace', 'TwoLeadECG', 'Two_Patterns', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y', 'uWaveGestureLibrary_Z', 'wafer', 'WordsSynonyms', 'yoga']\n",
    "\n",
    "# flist  = ['Adiac']\n",
    "# for each in flist:\n",
    "# fname = each\n",
    "\n",
    "# x_train, y_train = readucr(fname+'/'+fname+'_TRAIN')\n",
    "# x_test, y_test = readucr(fname+'/'+fname+'_TEST')\n",
    "nb_classes = len(np.unique(y_test))\n",
    "batch_size = min(X_train.shape[0]/10, 16)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "\n",
    "print(X_train.__class__.__name__)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean)/(X_train_std)\n",
    "\n",
    "X_test = (X_test - X_train_mean)/(X_train_std)\n",
    "X_train = X_train.reshape(X_train.shape + (1,1,))\n",
    "X_test = X_test.reshape(X_test.shape + (1,1,))\n",
    "\n",
    "x = keras.layers.Input(X_train.shape[1:])\n",
    "#    drop_out = Dropout(0.2)(x)\n",
    "conv1 = keras.layers.Conv2D(128, 8, 1, border_mode='same')(x)\n",
    "conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n",
    "conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv1)\n",
    "conv2 = keras.layers.Conv2D(256, 5, 1, border_mode='same')(conv1)\n",
    "conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n",
    "conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, 1, border_mode='same')(conv2)\n",
    "conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n",
    "conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "full = keras.layers.pooling.GlobalAveragePooling2D()(conv3)    \n",
    "out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "\n",
    "\n",
    "model = Model(input=x, output=out)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks = [reduce_lr])\n",
    "#Print the testing results which has the lowest training loss.\n",
    "log = pd.DataFrame(hist.history)\n",
    "print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
