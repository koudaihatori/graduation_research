{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/jdt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>commitdate</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2003/10/10 16:10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.581676</td>\n",
       "      <td>0.065669</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>690.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3709</td>\n",
       "      <td>1752.666667</td>\n",
       "      <td>3709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>2002/5/14 7:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>670.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13391</td>\n",
       "      <td>7983.500000</td>\n",
       "      <td>13391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>2002/5/13 15:48</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508.000000</td>\n",
       "      <td>1508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>2002/4/5 8:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>458</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12804</td>\n",
       "      <td>8025.500000</td>\n",
       "      <td>12804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>2001/8/24 15:14</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>429</td>\n",
       "      <td>0.622239</td>\n",
       "      <td>0.217704</td>\n",
       "      <td>0.266461</td>\n",
       "      <td>170.533800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>835</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>6003</td>\n",
       "      <td>6003.000000</td>\n",
       "      <td>6003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transactionid        commitdate  ns  nm   nf   entropy        la        ld  \\\n",
       "0              7  2003/10/10 16:10   1   1    3  0.581676  0.065669  0.021729   \n",
       "1             13    2002/5/14 7:42   1   1    1  0.000000  0.002985  0.008955   \n",
       "2             14   2002/5/13 15:48   1   2    2  0.000000  0.000000  0.000000   \n",
       "3             16     2002/4/5 8:48   1   1    3  0.000000  0.000000  0.000000   \n",
       "4             28   2001/8/24 15:14   1  37  429  0.622239  0.217704  0.266461   \n",
       "\n",
       "           lt  fix  ndev   pd       npt    exp         rexp   sexp  bug  \n",
       "0  690.333333    1     6   36  0.666667   3709  1752.666667   3709    1  \n",
       "1  670.000000    0     3    0  1.000000  13391  7983.500000  13391    0  \n",
       "2   34.500000    0     1    7  1.000000   1508  1508.000000   1508    1  \n",
       "3    0.000000    0     1  458  0.666667  12804  8025.500000  12804    0  \n",
       "4  170.533800    0     1  835  0.018648   6003  6003.000000   6003    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X.drop(\"commitdate\", axis =1, inplace=True)\n",
    "y = df.iloc[:, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24770, 15)\n",
      "(10616, 15)\n",
      "(24770,)\n",
      "(10616,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ロジスティック回帰の混合行列 [[9068    6]\n",
      " [1539    3]]\n",
      "ロジスティック回帰での正答率 0.8544649585531273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_p_lr = lr.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_lr))\n",
    "\n",
    "print (\"ロジスティック回帰での正答率\", accuracy_score(y_test, y_p_lr))\n",
    "#ロジスティック回帰正答率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMでの正答率 0.8463639788997739\n",
      "ロジスティック回帰の混合行列 [[8924  150]\n",
      " [1481   61]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_p_svm = svm.predict(X_test)\n",
    "# 正答率を算出\n",
    "print('SVMでの正答率', accuracy_score(y_test, y_p_svm))\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knnの混合行列 [[8864  210]\n",
      " [1440  102]]\n",
      "knnでの正答率 0.8445742275810098\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "list_nn = []\n",
    "list_score = []\n",
    "for k in range(1, 5):\n",
    "    knc = KNeighborsClassifier(n_neighbors=k)\n",
    "    knc.fit(X_train, y_train)\n",
    "    Y_pred = knc.predict(X_test)\n",
    "    score = knc.score(X_test, y_test)\n",
    "#     print(\"[%d] score: {:.2f}\".format(score) % k)\n",
    "    list_nn.append(k)\n",
    "    list_score.append(score)\n",
    "\n",
    "#プロット\n",
    "plt.ylim(0.1, 1.0)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(list_nn, list_score)\n",
    "\n",
    "#k = 2の時が良い\n",
    "knc = KNeighborsClassifier(n_neighbors=2)\n",
    "knc.fit(X_train, y_train)\n",
    "Y_pred = knc.predict(X_test)\n",
    "score = knc.score(X_test, y_test)\n",
    "print(\"knnの混合行列\", confusion_matrix(y_test, Y_pred))\n",
    "print (\"knnでの正答率\", accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.convolutional import Conv1D, UpSampling1D\n",
    "from keras.layers.pooling import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Add, Input\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# lookback = 5\n",
    "\n",
    "# #データを4次元化\n",
    "# X_train = X_train.reshape((len(X_train),lookback,1,1))\n",
    "# X_val = X_val.reshape((len(X_val),lookback,1,1))\n",
    "# X_test = X_test.reshape((len(X_test),lookback,1,1))\n",
    "\n",
    "# #CNNの学習\n",
    "# input_ = Input(shape=(lookback, 1,1))#横の数、縦の数、RGB\n",
    "\n",
    "# c = Conv2D(8, (3, 1),padding='same',activation='relu')(input_)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = MaxPooling2D(pool_size=(2, 1))(c)\n",
    "\n",
    "# c = Flatten()(c)\n",
    "# c = Dense(30,activation='relu')(c)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = Dense(4, activation='softmax')(c)\n",
    "\n",
    "# model = Model(input_, c)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# hist = model.fit(X_train, Y_train, batch_size = 10, epochs=100, verbose=1, shuffle=True,\n",
    "#                  validation_data = (X_val,Y_val))\n",
    "# #結果描画\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_loss'],label=\"val_loss\")\n",
    "# plt.plot(hist.history['loss'],label=\"train_loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_acc'],label=\"val_acc\")\n",
    "# plt.plot(hist.history['acc'],label=\"train_acc\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24770 samples, validate on 10616 samples\n",
      "Epoch 1/200\n",
      "24770/24770 [==============================] - 26s 1ms/step - loss: 0.4136 - acc: 0.8558 - val_loss: 0.4119 - val_acc: 0.8547\n",
      "Epoch 2/200\n",
      "24770/24770 [==============================] - 25s 992us/step - loss: 0.4087 - acc: 0.8568 - val_loss: 0.4102 - val_acc: 0.8547\n",
      "Epoch 3/200\n",
      "24770/24770 [==============================] - 25s 993us/step - loss: 0.4069 - acc: 0.8568 - val_loss: 0.4116 - val_acc: 0.8547\n",
      "Epoch 4/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.4065 - acc: 0.8568 - val_loss: 0.4102 - val_acc: 0.8547\n",
      "Epoch 5/200\n",
      "24770/24770 [==============================] - 24s 987us/step - loss: 0.4049 - acc: 0.8568 - val_loss: 0.4155 - val_acc: 0.8547\n",
      "Epoch 6/200\n",
      "24770/24770 [==============================] - 24s 977us/step - loss: 0.4038 - acc: 0.8568 - val_loss: 0.4105 - val_acc: 0.8547\n",
      "Epoch 7/200\n",
      "24770/24770 [==============================] - 24s 987us/step - loss: 0.4026 - acc: 0.8568 - val_loss: 0.4042 - val_acc: 0.8547\n",
      "Epoch 8/200\n",
      "24770/24770 [==============================] - 24s 986us/step - loss: 0.4020 - acc: 0.8568 - val_loss: 0.4051 - val_acc: 0.8547\n",
      "Epoch 9/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.4011 - acc: 0.8568 - val_loss: 0.4132 - val_acc: 0.8547\n",
      "Epoch 10/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.4004 - acc: 0.8568 - val_loss: 0.4058 - val_acc: 0.8547\n",
      "Epoch 11/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.4000 - acc: 0.8568 - val_loss: 0.4774 - val_acc: 0.8547\n",
      "Epoch 12/200\n",
      "24770/24770 [==============================] - 25s 997us/step - loss: 0.4003 - acc: 0.8568 - val_loss: 0.3988 - val_acc: 0.8547\n",
      "Epoch 13/200\n",
      "24770/24770 [==============================] - 25s 999us/step - loss: 0.3991 - acc: 0.8569 - val_loss: 0.4108 - val_acc: 0.8547\n",
      "Epoch 14/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3985 - acc: 0.8568 - val_loss: 0.4031 - val_acc: 0.8547\n",
      "Epoch 15/200\n",
      "24770/24770 [==============================] - 25s 992us/step - loss: 0.3983 - acc: 0.8569 - val_loss: 0.4129 - val_acc: 0.8542\n",
      "Epoch 16/200\n",
      "24770/24770 [==============================] - 25s 997us/step - loss: 0.3984 - acc: 0.8568 - val_loss: 0.3970 - val_acc: 0.8547\n",
      "Epoch 17/200\n",
      "24770/24770 [==============================] - 24s 983us/step - loss: 0.3971 - acc: 0.8567 - val_loss: 0.4343 - val_acc: 0.8547\n",
      "Epoch 18/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3962 - acc: 0.8568 - val_loss: 0.3991 - val_acc: 0.8547\n",
      "Epoch 19/200\n",
      "24770/24770 [==============================] - 25s 995us/step - loss: 0.3968 - acc: 0.8568 - val_loss: 0.4417 - val_acc: 0.8539\n",
      "Epoch 20/200\n",
      "24770/24770 [==============================] - 25s 995us/step - loss: 0.3963 - acc: 0.8568 - val_loss: 0.3943 - val_acc: 0.8547\n",
      "Epoch 21/200\n",
      "24770/24770 [==============================] - 24s 962us/step - loss: 0.3957 - acc: 0.8569 - val_loss: 0.4084 - val_acc: 0.8547\n",
      "Epoch 22/200\n",
      "24770/24770 [==============================] - 25s 995us/step - loss: 0.3959 - acc: 0.8569 - val_loss: 0.3963 - val_acc: 0.8547\n",
      "Epoch 23/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3947 - acc: 0.8569 - val_loss: 0.4176 - val_acc: 0.8547\n",
      "Epoch 24/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3953 - acc: 0.8568 - val_loss: 0.3981 - val_acc: 0.8547\n",
      "Epoch 25/200\n",
      "24770/24770 [==============================] - 25s 994us/step - loss: 0.3952 - acc: 0.8568 - val_loss: 0.4058 - val_acc: 0.8547\n",
      "Epoch 26/200\n",
      "24770/24770 [==============================] - 25s 998us/step - loss: 0.3937 - acc: 0.8568 - val_loss: 0.4377 - val_acc: 0.8547\n",
      "Epoch 27/200\n",
      "24770/24770 [==============================] - 25s 997us/step - loss: 0.3937 - acc: 0.8568 - val_loss: 0.4097 - val_acc: 0.8547\n",
      "Epoch 28/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3939 - acc: 0.8570 - val_loss: 0.3992 - val_acc: 0.8547\n",
      "Epoch 29/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3934 - acc: 0.8568 - val_loss: 0.3937 - val_acc: 0.8547\n",
      "Epoch 30/200\n",
      "24770/24770 [==============================] - 25s 991us/step - loss: 0.3936 - acc: 0.8568 - val_loss: 0.4252 - val_acc: 0.8547\n",
      "Epoch 31/200\n",
      "24770/24770 [==============================] - 30s 1ms/step - loss: 0.3932 - acc: 0.8569 - val_loss: 0.3992 - val_acc: 0.8547\n",
      "Epoch 32/200\n",
      "24770/24770 [==============================] - 28s 1ms/step - loss: 0.3927 - acc: 0.8568 - val_loss: 0.3998 - val_acc: 0.8547\n",
      "Epoch 33/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3928 - acc: 0.8568 - val_loss: 0.4072 - val_acc: 0.8547\n",
      "Epoch 34/200\n",
      "24770/24770 [==============================] - 28s 1ms/step - loss: 0.3913 - acc: 0.8568 - val_loss: 0.4052 - val_acc: 0.8546\n",
      "Epoch 35/200\n",
      "24770/24770 [==============================] - 28s 1ms/step - loss: 0.3922 - acc: 0.8568 - val_loss: 0.4494 - val_acc: 0.8487\n",
      "Epoch 36/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3913 - acc: 0.8568 - val_loss: 0.3984 - val_acc: 0.8547\n",
      "Epoch 37/200\n",
      "24770/24770 [==============================] - 31s 1ms/step - loss: 0.3919 - acc: 0.8570 - val_loss: 0.4108 - val_acc: 0.8544\n",
      "Epoch 38/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3902 - acc: 0.8569 - val_loss: 0.4090 - val_acc: 0.8547\n",
      "Epoch 39/200\n",
      "24770/24770 [==============================] - 30s 1ms/step - loss: 0.3902 - acc: 0.8569 - val_loss: 0.3935 - val_acc: 0.8547\n",
      "Epoch 40/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3904 - acc: 0.8569 - val_loss: 0.3954 - val_acc: 0.8547\n",
      "Epoch 41/200\n",
      "24770/24770 [==============================] - 32s 1ms/step - loss: 0.3906 - acc: 0.8567 - val_loss: 0.4431 - val_acc: 0.8534\n",
      "Epoch 42/200\n",
      "24770/24770 [==============================] - 28s 1ms/step - loss: 0.3901 - acc: 0.8567 - val_loss: 0.4156 - val_acc: 0.8545\n",
      "Epoch 43/200\n",
      "24770/24770 [==============================] - 28s 1ms/step - loss: 0.3902 - acc: 0.8570 - val_loss: 0.4016 - val_acc: 0.8546\n",
      "Epoch 44/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3902 - acc: 0.8569 - val_loss: 0.3882 - val_acc: 0.8545\n",
      "Epoch 45/200\n",
      "24770/24770 [==============================] - 30s 1ms/step - loss: 0.3886 - acc: 0.8567 - val_loss: 0.3987 - val_acc: 0.8552\n",
      "Epoch 46/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3886 - acc: 0.8569 - val_loss: 0.3906 - val_acc: 0.8551\n",
      "Epoch 47/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3887 - acc: 0.8570 - val_loss: 0.4095 - val_acc: 0.8543\n",
      "Epoch 48/200\n",
      "24770/24770 [==============================] - 37s 1ms/step - loss: 0.3895 - acc: 0.8569 - val_loss: 0.3972 - val_acc: 0.8538\n",
      "Epoch 49/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3884 - acc: 0.8569 - val_loss: 0.4142 - val_acc: 0.8545\n",
      "Epoch 50/200\n",
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3882 - acc: 0.8572 - val_loss: 0.4018 - val_acc: 0.8550\n",
      "Epoch 51/200\n",
      "24770/24770 [==============================] - 35s 1ms/step - loss: 0.3882 - acc: 0.8570 - val_loss: 0.4059 - val_acc: 0.8547\n",
      "Epoch 52/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3873 - acc: 0.8570 - val_loss: 0.4394 - val_acc: 0.8480\n",
      "Epoch 53/200\n",
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3874 - acc: 0.8572 - val_loss: 0.4231 - val_acc: 0.8534\n",
      "Epoch 54/200\n",
      "24770/24770 [==============================] - 32s 1ms/step - loss: 0.3875 - acc: 0.8568 - val_loss: 0.4030 - val_acc: 0.8546\n",
      "Epoch 55/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3876 - acc: 0.8570 - val_loss: 0.3890 - val_acc: 0.8547\n",
      "Epoch 56/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3874 - acc: 0.8569 - val_loss: 0.3967 - val_acc: 0.8547\n",
      "Epoch 57/200\n",
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3870 - acc: 0.8574 - val_loss: 0.3920 - val_acc: 0.8547\n",
      "Epoch 58/200\n",
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3868 - acc: 0.8569 - val_loss: 0.3894 - val_acc: 0.8548\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3871 - acc: 0.8568 - val_loss: 0.3941 - val_acc: 0.8551\n",
      "Epoch 60/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3867 - acc: 0.8567 - val_loss: 0.4040 - val_acc: 0.8540\n",
      "Epoch 61/200\n",
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3862 - acc: 0.8571 - val_loss: 0.3892 - val_acc: 0.8551\n",
      "Epoch 62/200\n",
      "24770/24770 [==============================] - 37s 1ms/step - loss: 0.3860 - acc: 0.8571 - val_loss: 0.4254 - val_acc: 0.8537\n",
      "Epoch 63/200\n",
      "24770/24770 [==============================] - 30s 1ms/step - loss: 0.3866 - acc: 0.8568 - val_loss: 0.3950 - val_acc: 0.8550\n",
      "Epoch 64/200\n",
      "24770/24770 [==============================] - 27s 1ms/step - loss: 0.3861 - acc: 0.8568 - val_loss: 0.4310 - val_acc: 0.8527\n",
      "Epoch 65/200\n",
      "24770/24770 [==============================] - 28s 1ms/step - loss: 0.3858 - acc: 0.8570 - val_loss: 0.4150 - val_acc: 0.8525\n",
      "Epoch 66/200\n",
      "24770/24770 [==============================] - 27s 1ms/step - loss: 0.3857 - acc: 0.8569 - val_loss: 0.4022 - val_acc: 0.8547\n",
      "Epoch 67/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3863 - acc: 0.8570 - val_loss: 0.4347 - val_acc: 0.8455\n",
      "Epoch 68/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3856 - acc: 0.8574 - val_loss: 0.3961 - val_acc: 0.8554\n",
      "Epoch 69/200\n",
      "24770/24770 [==============================] - 24s 959us/step - loss: 0.3857 - acc: 0.8571 - val_loss: 0.4044 - val_acc: 0.8545\n",
      "Epoch 70/200\n",
      "24770/24770 [==============================] - 24s 959us/step - loss: 0.3853 - acc: 0.8572 - val_loss: 0.4107 - val_acc: 0.8550\n",
      "Epoch 71/200\n",
      "24770/24770 [==============================] - 24s 985us/step - loss: 0.3855 - acc: 0.8570 - val_loss: 0.4091 - val_acc: 0.8555\n",
      "Epoch 72/200\n",
      "24770/24770 [==============================] - 24s 957us/step - loss: 0.3851 - acc: 0.8568 - val_loss: 0.4594 - val_acc: 0.8547\n",
      "Epoch 73/200\n",
      "24770/24770 [==============================] - 24s 957us/step - loss: 0.3852 - acc: 0.8571 - val_loss: 0.4171 - val_acc: 0.8498\n",
      "Epoch 74/200\n",
      "24770/24770 [==============================] - 24s 960us/step - loss: 0.3850 - acc: 0.8572 - val_loss: 0.4100 - val_acc: 0.8547\n",
      "Epoch 75/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3846 - acc: 0.8575 - val_loss: 0.3993 - val_acc: 0.8549\n",
      "Epoch 76/200\n",
      "24770/24770 [==============================] - 24s 989us/step - loss: 0.3843 - acc: 0.8568 - val_loss: 0.3890 - val_acc: 0.8551\n",
      "Epoch 77/200\n",
      "24770/24770 [==============================] - 24s 957us/step - loss: 0.3841 - acc: 0.8575 - val_loss: 0.4014 - val_acc: 0.8541\n",
      "Epoch 78/200\n",
      "24770/24770 [==============================] - 24s 954us/step - loss: 0.3838 - acc: 0.8572 - val_loss: 0.3962 - val_acc: 0.8552\n",
      "Epoch 79/200\n",
      "24770/24770 [==============================] - 24s 954us/step - loss: 0.3841 - acc: 0.8573 - val_loss: 0.3950 - val_acc: 0.8555\n",
      "Epoch 80/200\n",
      "24770/24770 [==============================] - 24s 959us/step - loss: 0.3845 - acc: 0.8569 - val_loss: 0.4103 - val_acc: 0.8531\n",
      "Epoch 81/200\n",
      "24770/24770 [==============================] - 25s 995us/step - loss: 0.3838 - acc: 0.8570 - val_loss: 0.3871 - val_acc: 0.8556\n",
      "Epoch 82/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3835 - acc: 0.8576 - val_loss: 0.4341 - val_acc: 0.8520\n",
      "Epoch 83/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3837 - acc: 0.8572 - val_loss: 0.4282 - val_acc: 0.8495\n",
      "Epoch 84/200\n",
      "24770/24770 [==============================] - 24s 963us/step - loss: 0.3848 - acc: 0.8572 - val_loss: 0.4219 - val_acc: 0.8521\n",
      "Epoch 85/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3841 - acc: 0.8572 - val_loss: 0.4081 - val_acc: 0.8539\n",
      "Epoch 86/200\n",
      "24770/24770 [==============================] - 24s 988us/step - loss: 0.3835 - acc: 0.8575 - val_loss: 0.4096 - val_acc: 0.8515\n",
      "Epoch 87/200\n",
      "24770/24770 [==============================] - 24s 957us/step - loss: 0.3833 - acc: 0.8574 - val_loss: 0.3972 - val_acc: 0.8551\n",
      "Epoch 88/200\n",
      "24770/24770 [==============================] - 24s 957us/step - loss: 0.3827 - acc: 0.8578 - val_loss: 0.5332 - val_acc: 0.7730\n",
      "Epoch 89/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3830 - acc: 0.8569 - val_loss: 0.3982 - val_acc: 0.8551\n",
      "Epoch 90/200\n",
      "24770/24770 [==============================] - 24s 961us/step - loss: 0.3825 - acc: 0.8573 - val_loss: 0.4661 - val_acc: 0.8158\n",
      "Epoch 91/200\n",
      "24770/24770 [==============================] - 24s 988us/step - loss: 0.3821 - acc: 0.8576 - val_loss: 0.3913 - val_acc: 0.8552\n",
      "Epoch 92/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3828 - acc: 0.8578 - val_loss: 0.4240 - val_acc: 0.8481\n",
      "Epoch 93/200\n",
      "24770/24770 [==============================] - 24s 960us/step - loss: 0.3820 - acc: 0.8578 - val_loss: 0.4318 - val_acc: 0.8486\n",
      "Epoch 94/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3828 - acc: 0.8573 - val_loss: 0.4273 - val_acc: 0.8481\n",
      "Epoch 95/200\n",
      "24770/24770 [==============================] - 24s 959us/step - loss: 0.3823 - acc: 0.8577 - val_loss: 0.3945 - val_acc: 0.8547\n",
      "Epoch 96/200\n",
      "24770/24770 [==============================] - 24s 985us/step - loss: 0.3816 - acc: 0.8576 - val_loss: 0.4372 - val_acc: 0.8451\n",
      "Epoch 97/200\n",
      "24770/24770 [==============================] - 24s 957us/step - loss: 0.3823 - acc: 0.8580 - val_loss: 0.4091 - val_acc: 0.8524\n",
      "Epoch 98/200\n",
      "24770/24770 [==============================] - 24s 955us/step - loss: 0.3822 - acc: 0.8576 - val_loss: 0.3998 - val_acc: 0.8555\n",
      "Epoch 99/200\n",
      "24770/24770 [==============================] - 24s 961us/step - loss: 0.3819 - acc: 0.8576 - val_loss: 0.3922 - val_acc: 0.8555\n",
      "Epoch 100/200\n",
      "24770/24770 [==============================] - 24s 958us/step - loss: 0.3813 - acc: 0.8576 - val_loss: 0.4163 - val_acc: 0.8520\n",
      "Epoch 101/200\n",
      "24770/24770 [==============================] - 25s 996us/step - loss: 0.3812 - acc: 0.8577 - val_loss: 0.5287 - val_acc: 0.7638\n",
      "Epoch 102/200\n",
      "24770/24770 [==============================] - 24s 966us/step - loss: 0.3802 - acc: 0.8581 - val_loss: 0.4686 - val_acc: 0.8159\n",
      "Epoch 103/200\n",
      "24770/24770 [==============================] - 24s 969us/step - loss: 0.3819 - acc: 0.8577 - val_loss: 0.4050 - val_acc: 0.8498\n",
      "Epoch 104/200\n",
      "24770/24770 [==============================] - 24s 969us/step - loss: 0.3804 - acc: 0.8577 - val_loss: 0.4201 - val_acc: 0.8418\n",
      "Epoch 105/200\n",
      "24770/24770 [==============================] - 24s 969us/step - loss: 0.3808 - acc: 0.8576 - val_loss: 0.4430 - val_acc: 0.8479\n",
      "Epoch 106/200\n",
      "24770/24770 [==============================] - 25s 998us/step - loss: 0.3808 - acc: 0.8572 - val_loss: 0.3934 - val_acc: 0.8556\n",
      "Epoch 107/200\n",
      "24770/24770 [==============================] - 24s 968us/step - loss: 0.3809 - acc: 0.8577 - val_loss: 0.4891 - val_acc: 0.8101\n",
      "Epoch 108/200\n",
      "24770/24770 [==============================] - 24s 965us/step - loss: 0.3808 - acc: 0.8577 - val_loss: 0.4191 - val_acc: 0.8456\n",
      "Epoch 109/200\n",
      "24770/24770 [==============================] - 24s 965us/step - loss: 0.3809 - acc: 0.8577 - val_loss: 0.4656 - val_acc: 0.8311\n",
      "Epoch 110/200\n",
      "24770/24770 [==============================] - 24s 963us/step - loss: 0.3802 - acc: 0.8575 - val_loss: 0.6989 - val_acc: 0.5859\n",
      "Epoch 111/200\n",
      "24770/24770 [==============================] - 25s 993us/step - loss: 0.3811 - acc: 0.8582 - val_loss: 0.4248 - val_acc: 0.8525\n",
      "Epoch 112/200\n",
      "24770/24770 [==============================] - 26s 1ms/step - loss: 0.3801 - acc: 0.8579 - val_loss: 0.4422 - val_acc: 0.8491\n",
      "Epoch 113/200\n",
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3794 - acc: 0.8573 - val_loss: 0.4449 - val_acc: 0.8384\n",
      "Epoch 114/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3796 - acc: 0.8579 - val_loss: 0.4236 - val_acc: 0.8497\n",
      "Epoch 115/200\n",
      "24770/24770 [==============================] - 42s 2ms/step - loss: 0.3803 - acc: 0.8579 - val_loss: 0.4098 - val_acc: 0.8518\n",
      "Epoch 116/200\n",
      "24770/24770 [==============================] - 39s 2ms/step - loss: 0.3794 - acc: 0.8577 - val_loss: 0.3927 - val_acc: 0.8562\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3794 - acc: 0.8582 - val_loss: 0.3938 - val_acc: 0.8558\n",
      "Epoch 118/200\n",
      "24770/24770 [==============================] - 30s 1ms/step - loss: 0.3798 - acc: 0.8577 - val_loss: 0.4408 - val_acc: 0.8476\n",
      "Epoch 119/200\n",
      "24770/24770 [==============================] - 24s 985us/step - loss: 0.3787 - acc: 0.8578 - val_loss: 0.4265 - val_acc: 0.8455\n",
      "Epoch 120/200\n",
      "24770/24770 [==============================] - 26s 1ms/step - loss: 0.3789 - acc: 0.8575 - val_loss: 0.4541 - val_acc: 0.8365\n",
      "Epoch 121/200\n",
      "24770/24770 [==============================] - 25s 995us/step - loss: 0.3786 - acc: 0.8582 - val_loss: 0.3949 - val_acc: 0.8547\n",
      "Epoch 122/200\n",
      "24770/24770 [==============================] - 25s 992us/step - loss: 0.3785 - acc: 0.8581 - val_loss: 0.4805 - val_acc: 0.8234\n",
      "Epoch 123/200\n",
      "24770/24770 [==============================] - 25s 998us/step - loss: 0.3791 - acc: 0.8576 - val_loss: 0.8033 - val_acc: 0.4665\n",
      "Epoch 124/200\n",
      "24770/24770 [==============================] - 25s 996us/step - loss: 0.3786 - acc: 0.8580 - val_loss: 0.3932 - val_acc: 0.8557\n",
      "Epoch 125/200\n",
      "24770/24770 [==============================] - 24s 987us/step - loss: 0.3786 - acc: 0.8581 - val_loss: 0.4139 - val_acc: 0.8519\n",
      "Epoch 126/200\n",
      "24770/24770 [==============================] - 24s 967us/step - loss: 0.3788 - acc: 0.8574 - val_loss: 0.4132 - val_acc: 0.8547\n",
      "Epoch 127/200\n",
      "24770/24770 [==============================] - 24s 988us/step - loss: 0.3788 - acc: 0.8581 - val_loss: 0.4076 - val_acc: 0.8519\n",
      "Epoch 128/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3788 - acc: 0.8580 - val_loss: 0.4371 - val_acc: 0.8420\n",
      "Epoch 129/200\n",
      "24770/24770 [==============================] - 25s 991us/step - loss: 0.3782 - acc: 0.8581 - val_loss: 0.3929 - val_acc: 0.8559\n",
      "Epoch 130/200\n",
      "24770/24770 [==============================] - 25s 989us/step - loss: 0.3779 - acc: 0.8579 - val_loss: 0.4131 - val_acc: 0.8499\n",
      "Epoch 131/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3776 - acc: 0.8576 - val_loss: 0.4105 - val_acc: 0.8540\n",
      "Epoch 132/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3781 - acc: 0.8584 - val_loss: 0.3953 - val_acc: 0.8555\n",
      "Epoch 133/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3778 - acc: 0.8585 - val_loss: 0.4185 - val_acc: 0.8503\n",
      "Epoch 134/200\n",
      "24770/24770 [==============================] - 24s 974us/step - loss: 0.3778 - acc: 0.8572 - val_loss: 0.4365 - val_acc: 0.8458\n",
      "Epoch 135/200\n",
      "24770/24770 [==============================] - 25s 996us/step - loss: 0.3770 - acc: 0.8577 - val_loss: 0.5190 - val_acc: 0.7448\n",
      "Epoch 136/200\n",
      "24770/24770 [==============================] - 25s 995us/step - loss: 0.3771 - acc: 0.8576 - val_loss: 0.4374 - val_acc: 0.8394\n",
      "Epoch 137/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3780 - acc: 0.8577 - val_loss: 0.4880 - val_acc: 0.7954\n",
      "Epoch 138/200\n",
      "24770/24770 [==============================] - 24s 973us/step - loss: 0.3767 - acc: 0.8581 - val_loss: 0.4982 - val_acc: 0.7782\n",
      "Epoch 139/200\n",
      "24770/24770 [==============================] - 25s 989us/step - loss: 0.3778 - acc: 0.8581 - val_loss: 0.3997 - val_acc: 0.8529\n",
      "Epoch 140/200\n",
      "24770/24770 [==============================] - 24s 979us/step - loss: 0.3771 - acc: 0.8578 - val_loss: 0.4140 - val_acc: 0.8550\n",
      "Epoch 141/200\n",
      "24770/24770 [==============================] - 25s 996us/step - loss: 0.3770 - acc: 0.8581 - val_loss: 0.3920 - val_acc: 0.8553\n",
      "Epoch 142/200\n",
      "24770/24770 [==============================] - 25s 994us/step - loss: 0.3766 - acc: 0.8580 - val_loss: 0.3901 - val_acc: 0.8547\n",
      "Epoch 143/200\n",
      "24770/24770 [==============================] - 25s 998us/step - loss: 0.3760 - acc: 0.8582 - val_loss: 0.4711 - val_acc: 0.8061\n",
      "Epoch 144/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3771 - acc: 0.8579 - val_loss: 0.7084 - val_acc: 0.5503\n",
      "Epoch 145/200\n",
      "24770/24770 [==============================] - 25s 992us/step - loss: 0.3761 - acc: 0.8582 - val_loss: 0.4367 - val_acc: 0.8401\n",
      "Epoch 146/200\n",
      "24770/24770 [==============================] - 25s 994us/step - loss: 0.3760 - acc: 0.8587 - val_loss: 0.4531 - val_acc: 0.8293\n",
      "Epoch 147/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3763 - acc: 0.8582 - val_loss: 0.4509 - val_acc: 0.8311\n",
      "Epoch 148/200\n",
      "24770/24770 [==============================] - 25s 998us/step - loss: 0.3756 - acc: 0.8582 - val_loss: 0.5116 - val_acc: 0.7684\n",
      "Epoch 149/200\n",
      "24770/24770 [==============================] - 25s 991us/step - loss: 0.3759 - acc: 0.8583 - val_loss: 0.5654 - val_acc: 0.7224\n",
      "Epoch 150/200\n",
      "24770/24770 [==============================] - 25s 992us/step - loss: 0.3760 - acc: 0.8578 - val_loss: 0.4421 - val_acc: 0.8365\n",
      "Epoch 151/200\n",
      "24770/24770 [==============================] - 24s 980us/step - loss: 0.3750 - acc: 0.8576 - val_loss: 0.4061 - val_acc: 0.8537\n",
      "Epoch 152/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3760 - acc: 0.8587 - val_loss: 0.4247 - val_acc: 0.8418\n",
      "Epoch 153/200\n",
      "24770/24770 [==============================] - 24s 974us/step - loss: 0.3754 - acc: 0.8578 - val_loss: 0.4737 - val_acc: 0.8168\n",
      "Epoch 154/200\n",
      "24770/24770 [==============================] - 24s 976us/step - loss: 0.3751 - acc: 0.8587 - val_loss: 0.6028 - val_acc: 0.6908\n",
      "Epoch 155/200\n",
      "24770/24770 [==============================] - 25s 989us/step - loss: 0.3751 - acc: 0.8587 - val_loss: 0.4788 - val_acc: 0.8136\n",
      "Epoch 156/200\n",
      "24770/24770 [==============================] - 25s 991us/step - loss: 0.3748 - acc: 0.8581 - val_loss: 0.6756 - val_acc: 0.5873\n",
      "Epoch 157/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3744 - acc: 0.8586 - val_loss: 0.5010 - val_acc: 0.7776\n",
      "Epoch 158/200\n",
      "24770/24770 [==============================] - 25s 994us/step - loss: 0.3746 - acc: 0.8585 - val_loss: 0.5393 - val_acc: 0.7569\n",
      "Epoch 159/200\n",
      "24770/24770 [==============================] - 24s 989us/step - loss: 0.3746 - acc: 0.8577 - val_loss: 0.4001 - val_acc: 0.8548\n",
      "Epoch 160/200\n",
      "24770/24770 [==============================] - 25s 989us/step - loss: 0.3751 - acc: 0.8581 - val_loss: 0.5133 - val_acc: 0.7956\n",
      "Epoch 161/200\n",
      "24770/24770 [==============================] - 25s 999us/step - loss: 0.3741 - acc: 0.8581 - val_loss: 0.4580 - val_acc: 0.8282\n",
      "Epoch 162/200\n",
      "24770/24770 [==============================] - 25s 1ms/step - loss: 0.3745 - acc: 0.8589 - val_loss: 0.5278 - val_acc: 0.7464\n",
      "Epoch 163/200\n",
      "24770/24770 [==============================] - 25s 994us/step - loss: 0.3740 - acc: 0.8585 - val_loss: 0.4081 - val_acc: 0.8477\n",
      "Epoch 164/200\n",
      "24770/24770 [==============================] - 25s 998us/step - loss: 0.3738 - acc: 0.8589 - val_loss: 0.5463 - val_acc: 0.7380\n",
      "Epoch 165/200\n",
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3741 - acc: 0.8583 - val_loss: 0.4358 - val_acc: 0.8379\n",
      "Epoch 166/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3733 - acc: 0.8590 - val_loss: 1.0173 - val_acc: 0.2766\n",
      "Epoch 167/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3735 - acc: 0.8583 - val_loss: 0.4559 - val_acc: 0.8309\n",
      "Epoch 168/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3735 - acc: 0.8587 - val_loss: 1.5000 - val_acc: 0.2243\n",
      "Epoch 169/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3736 - acc: 0.8585 - val_loss: 0.8864 - val_acc: 0.3387\n",
      "Epoch 170/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3733 - acc: 0.8588 - val_loss: 1.5244 - val_acc: 0.1989\n",
      "Epoch 171/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3728 - acc: 0.8594 - val_loss: 1.2174 - val_acc: 0.2611\n",
      "Epoch 172/200\n",
      "24770/24770 [==============================] - 33s 1ms/step - loss: 0.3735 - acc: 0.8590 - val_loss: 0.5582 - val_acc: 0.7213\n",
      "Epoch 173/200\n",
      "24770/24770 [==============================] - 40s 2ms/step - loss: 0.3732 - acc: 0.8585 - val_loss: 0.8037 - val_acc: 0.4950\n",
      "Epoch 174/200\n",
      "24770/24770 [==============================] - 43s 2ms/step - loss: 0.3731 - acc: 0.8588 - val_loss: 0.7118 - val_acc: 0.5125\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3731 - acc: 0.8591 - val_loss: 0.6622 - val_acc: 0.6101\n",
      "Epoch 176/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3733 - acc: 0.8584 - val_loss: 0.4367 - val_acc: 0.8419\n",
      "Epoch 177/200\n",
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3718 - acc: 0.8589 - val_loss: 0.4472 - val_acc: 0.8317\n",
      "Epoch 178/200\n",
      "24770/24770 [==============================] - 32s 1ms/step - loss: 0.3716 - acc: 0.8595 - val_loss: 0.6567 - val_acc: 0.6155\n",
      "Epoch 179/200\n",
      "24770/24770 [==============================] - 31s 1ms/step - loss: 0.3725 - acc: 0.8585 - val_loss: 0.7048 - val_acc: 0.5686\n",
      "Epoch 180/200\n",
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3709 - acc: 0.8598 - val_loss: 0.4578 - val_acc: 0.8193\n",
      "Epoch 181/200\n",
      "24770/24770 [==============================] - 31s 1ms/step - loss: 0.3716 - acc: 0.8595 - val_loss: 1.1518 - val_acc: 0.2171\n",
      "Epoch 182/200\n",
      "24770/24770 [==============================] - 32s 1ms/step - loss: 0.3715 - acc: 0.8590 - val_loss: 0.8006 - val_acc: 0.4031\n",
      "Epoch 183/200\n",
      "24770/24770 [==============================] - 27s 1ms/step - loss: 0.3711 - acc: 0.8595 - val_loss: 0.7701 - val_acc: 0.5496\n",
      "Epoch 184/200\n",
      "24770/24770 [==============================] - 32s 1ms/step - loss: 0.3715 - acc: 0.8591 - val_loss: 0.4897 - val_acc: 0.7908\n",
      "Epoch 185/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3714 - acc: 0.8587 - val_loss: 1.0542 - val_acc: 0.2506\n",
      "Epoch 186/200\n",
      "24770/24770 [==============================] - 26s 1ms/step - loss: 0.3713 - acc: 0.8589 - val_loss: 1.8305 - val_acc: 0.1739\n",
      "Epoch 187/200\n",
      "24770/24770 [==============================] - 26s 1ms/step - loss: 0.3702 - acc: 0.8590 - val_loss: 0.6070 - val_acc: 0.6808\n",
      "Epoch 188/200\n",
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3703 - acc: 0.8591 - val_loss: 1.5545 - val_acc: 0.1988\n",
      "Epoch 189/200\n",
      "24770/24770 [==============================] - 38s 2ms/step - loss: 0.3715 - acc: 0.8585 - val_loss: 1.5956 - val_acc: 0.1837\n",
      "Epoch 190/200\n",
      "24770/24770 [==============================] - 35s 1ms/step - loss: 0.3698 - acc: 0.8593 - val_loss: 1.9114 - val_acc: 0.1610\n",
      "Epoch 191/200\n",
      "24770/24770 [==============================] - 31s 1ms/step - loss: 0.3701 - acc: 0.8595 - val_loss: 0.5462 - val_acc: 0.7337\n",
      "Epoch 192/200\n",
      "24770/24770 [==============================] - 30s 1ms/step - loss: 0.3705 - acc: 0.8589 - val_loss: 0.4897 - val_acc: 0.7958\n",
      "Epoch 193/200\n",
      "24770/24770 [==============================] - 27s 1ms/step - loss: 0.3698 - acc: 0.8589 - val_loss: 0.8619 - val_acc: 0.3442\n",
      "Epoch 194/200\n",
      "24770/24770 [==============================] - 29s 1ms/step - loss: 0.3690 - acc: 0.8593 - val_loss: 0.7150 - val_acc: 0.5682\n",
      "Epoch 195/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3694 - acc: 0.8592 - val_loss: 0.7716 - val_acc: 0.4603\n",
      "Epoch 196/200\n",
      "24770/24770 [==============================] - 36s 1ms/step - loss: 0.3692 - acc: 0.8592 - val_loss: 0.6046 - val_acc: 0.6788\n",
      "Epoch 197/200\n",
      "24770/24770 [==============================] - 43s 2ms/step - loss: 0.3692 - acc: 0.8595 - val_loss: 0.4203 - val_acc: 0.8474\n",
      "Epoch 198/200\n",
      "24770/24770 [==============================] - 44s 2ms/step - loss: 0.3696 - acc: 0.8595 - val_loss: 0.8837 - val_acc: 0.3321\n",
      "Epoch 199/200\n",
      "24770/24770 [==============================] - 34s 1ms/step - loss: 0.3683 - acc: 0.8602 - val_loss: 0.5433 - val_acc: 0.7126\n",
      "Epoch 200/200\n",
      "24770/24770 [==============================] - 32s 1ms/step - loss: 0.3670 - acc: 0.8601 - val_loss: 0.4419 - val_acc: 0.8188\n",
      "0.3669560238099358 0.8187641296156745\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import keras \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "      \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  \n",
    "nb_epochs = 200\n",
    "\n",
    "\n",
    "#flist = ['Adiac', 'Beef', 'CBF', 'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', \n",
    "#'DiatomSizeReduction', 'ECGFiveDays', 'FaceAll', 'FaceFour', 'FacesUCR', '50words', 'FISH', 'Gun_Point', 'Haptics', \n",
    "#'InlineSkate', 'ItalyPowerDemand', 'Lighting2', 'Lighting7', 'MALLAT', 'MedicalImages', 'MoteStrain', 'NonInvasiveFatalECG_Thorax1', \n",
    "#'NonInvasiveFatalECG_Thorax2', 'OliveOil', 'OSULeaf', 'SonyAIBORobotSurface', 'SonyAIBORobotSurfaceII', 'StarLightCurves', 'SwedishLeaf', 'Symbols', \n",
    "#'synthetic_control', 'Trace', 'TwoLeadECG', 'Two_Patterns', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y', 'uWaveGestureLibrary_Z', 'wafer', 'WordsSynonyms', 'yoga']\n",
    "\n",
    "# flist  = ['Adiac']\n",
    "# for each in flist:\n",
    "# fname = each\n",
    "\n",
    "# x_train, y_train = readucr(fname+'/'+fname+'_TRAIN')\n",
    "# x_test, y_test = readucr(fname+'/'+fname+'_TEST')\n",
    "nb_classes = len(np.unique(y_test))\n",
    "batch_size = min(X_train.shape[0]/10, 16)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "\n",
    "print(X_train.__class__.__name__)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean)/(X_train_std)\n",
    "\n",
    "X_test = (X_test - X_train_mean)/(X_train_std)\n",
    "X_train = X_train.reshape(X_train.shape + (1,1,))\n",
    "X_test = X_test.reshape(X_test.shape + (1,1,))\n",
    "\n",
    "x = keras.layers.Input(X_train.shape[1:])\n",
    "#    drop_out = Dropout(0.2)(x)\n",
    "conv1 = keras.layers.Conv2D(128, 8, 1, border_mode='same')(x)\n",
    "conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n",
    "conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv1)\n",
    "conv2 = keras.layers.Conv2D(256, 5, 1, border_mode='same')(conv1)\n",
    "conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n",
    "conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, 1, border_mode='same')(conv2)\n",
    "conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n",
    "conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "full = keras.layers.pooling.GlobalAveragePooling2D()(conv3)    \n",
    "out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "\n",
    "\n",
    "model = Model(input=x, output=out)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks = [reduce_lr])\n",
    "#Print the testing results which has the lowest training loss.\n",
    "log = pd.DataFrame(hist.history)\n",
    "print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
