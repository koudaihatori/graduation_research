{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/mozilla.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>commitdate</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>2006/8/28 11:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.234710</td>\n",
       "      <td>0.513648</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>403.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>423</td>\n",
       "      <td>153.983333</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>2006/7/31 14:11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.435613</td>\n",
       "      <td>0.214493</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>172.5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1107</td>\n",
       "      <td>1107.000000</td>\n",
       "      <td>598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>2006/6/22 17:30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037184</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>2006/5/25 11:45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>474</td>\n",
       "      <td>474.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>2005/12/7 20:38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7688</td>\n",
       "      <td>2898.733730</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transactionid       commitdate  ns  nm  nf   entropy        la        ld  \\\n",
       "0             56  2006/8/28 11:24   1   1   3  0.234710  0.513648  0.172043   \n",
       "1             61  2006/7/31 14:11   2   2   2  0.435613  0.214493  0.011594   \n",
       "2             62  2006/6/22 17:30   1   1   1  0.000000  0.037184  0.006941   \n",
       "3             66  2006/5/25 11:45   1   1   1  0.000000  0.044118  0.000000   \n",
       "4             97  2005/12/7 20:38   1   1   1  0.000000  0.002500  0.001250   \n",
       "\n",
       "       lt  fix  ndev   pd  npt   exp         rexp  sexp  bug  \n",
       "0   403.0    1    31  602  1.0   423   153.983333    21    0  \n",
       "1   172.5    1    13   59  1.0  1107  1107.000000   598    0  \n",
       "2  2017.0    1    80    1  1.0     8     8.000000     1    0  \n",
       "3   204.0    1     5   21  1.0   474   474.000000    30    0  \n",
       "4   800.0    1     5    2  1.0  7688  2898.733730    19    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X.drop(\"commitdate\", axis =1, inplace=True)\n",
    "y = df.iloc[:, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68792, 15)\n",
      "(29483, 15)\n",
      "(68792,)\n",
      "(29483,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ロジスティック回帰の混合行列 [[27958     1]\n",
      " [ 1524     0]]\n",
      "ロジスティック回帰での正答率 0.9482752772784316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_p_lr = lr.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_lr))\n",
    "\n",
    "print (\"ロジスティック回帰での正答率\", accuracy_score(y_test, y_p_lr))\n",
    "#ロジスティック回帰正答率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMでの正答率 0.9436963673981617\n",
      "ロジスティック回帰の混合行列 [[27706   253]\n",
      " [ 1407   117]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_p_svm = svm.predict(X_test)\n",
    "# 正答率を算出\n",
    "print('SVMでの正答率', accuracy_score(y_test, y_p_svm))\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knnの混合行列 [[27807   152]\n",
      " [ 1483    41]]\n",
      "knnでの正答率 0.9445443136722858\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "list_nn = []\n",
    "list_score = []\n",
    "for k in range(1, 5):\n",
    "    knc = KNeighborsClassifier(n_neighbors=k)\n",
    "    knc.fit(X_train, y_train)\n",
    "    Y_pred = knc.predict(X_test)\n",
    "    score = knc.score(X_test, y_test)\n",
    "#     print(\"[%d] score: {:.2f}\".format(score) % k)\n",
    "    list_nn.append(k)\n",
    "    list_score.append(score)\n",
    "\n",
    "#プロット\n",
    "plt.ylim(0.1, 1.0)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(list_nn, list_score)\n",
    "\n",
    "#k = 2の時が良い\n",
    "knc = KNeighborsClassifier(n_neighbors=2)\n",
    "knc.fit(X_train, y_train)\n",
    "Y_pred = knc.predict(X_test)\n",
    "score = knc.score(X_test, y_test)\n",
    "print(\"knnの混合行列\", confusion_matrix(y_test, Y_pred))\n",
    "print (\"knnでの正答率\", accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.convolutional import Conv1D, UpSampling1D\n",
    "from keras.layers.pooling import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Add, Input\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# lookback = 5\n",
    "\n",
    "# #データを4次元化\n",
    "# X_train = X_train.reshape((len(X_train),lookback,1,1))\n",
    "# X_val = X_val.reshape((len(X_val),lookback,1,1))\n",
    "# X_test = X_test.reshape((len(X_test),lookback,1,1))\n",
    "\n",
    "# #CNNの学習\n",
    "# input_ = Input(shape=(lookback, 1,1))#横の数、縦の数、RGB\n",
    "\n",
    "# c = Conv2D(8, (3, 1),padding='same',activation='relu')(input_)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = MaxPooling2D(pool_size=(2, 1))(c)\n",
    "\n",
    "# c = Flatten()(c)\n",
    "# c = Dense(30,activation='relu')(c)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = Dense(4, activation='softmax')(c)\n",
    "\n",
    "# model = Model(input_, c)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# hist = model.fit(X_train, Y_train, batch_size = 10, epochs=100, verbose=1, shuffle=True,\n",
    "#                  validation_data = (X_val,Y_val))\n",
    "# #結果描画\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_loss'],label=\"val_loss\")\n",
    "# plt.plot(hist.history['loss'],label=\"train_loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_acc'],label=\"val_acc\")\n",
    "# plt.plot(hist.history['acc'],label=\"train_acc\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68792 samples, validate on 29483 samples\n",
      "Epoch 1/200\n",
      "68792/68792 [==============================] - 68s 987us/step - loss: 0.2076 - acc: 0.9473 - val_loss: 0.2277 - val_acc: 0.9483\n",
      "Epoch 2/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.2050 - acc: 0.9473 - val_loss: 0.2550 - val_acc: 0.9483\n",
      "Epoch 3/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.2038 - acc: 0.9473 - val_loss: 0.2003 - val_acc: 0.9483\n",
      "Epoch 4/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.2031 - acc: 0.9473 - val_loss: 0.2017 - val_acc: 0.9483\n",
      "Epoch 5/200\n",
      "68792/68792 [==============================] - 66s 966us/step - loss: 0.2025 - acc: 0.9473 - val_loss: 0.2031 - val_acc: 0.9483\n",
      "Epoch 6/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.2018 - acc: 0.9473 - val_loss: 0.2472 - val_acc: 0.9483\n",
      "Epoch 7/200\n",
      "68792/68792 [==============================] - 66s 962us/step - loss: 0.2018 - acc: 0.9473 - val_loss: 0.2270 - val_acc: 0.9483\n",
      "Epoch 8/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.2007 - acc: 0.9473 - val_loss: 0.2289 - val_acc: 0.9483\n",
      "Epoch 9/200\n",
      "68792/68792 [==============================] - 66s 964us/step - loss: 0.1999 - acc: 0.9473 - val_loss: 0.3170 - val_acc: 0.9483\n",
      "Epoch 10/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1996 - acc: 0.9473 - val_loss: 0.2069 - val_acc: 0.9483\n",
      "Epoch 11/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1988 - acc: 0.9473 - val_loss: 0.3059 - val_acc: 0.9483\n",
      "Epoch 12/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1987 - acc: 0.9473 - val_loss: 0.3094 - val_acc: 0.9483\n",
      "Epoch 13/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1988 - acc: 0.9473 - val_loss: 0.3335 - val_acc: 0.9483\n",
      "Epoch 14/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1982 - acc: 0.9473 - val_loss: 0.3178 - val_acc: 0.9483\n",
      "Epoch 15/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1971 - acc: 0.9473 - val_loss: 0.2755 - val_acc: 0.9483\n",
      "Epoch 16/200\n",
      "68792/68792 [==============================] - 66s 963us/step - loss: 0.1971 - acc: 0.9473 - val_loss: 0.2067 - val_acc: 0.9483\n",
      "Epoch 17/200\n",
      "68792/68792 [==============================] - 68s 984us/step - loss: 0.1967 - acc: 0.9473 - val_loss: 0.1928 - val_acc: 0.9483\n",
      "Epoch 18/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1960 - acc: 0.9473 - val_loss: 0.2287 - val_acc: 0.9483\n",
      "Epoch 19/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1954 - acc: 0.9473 - val_loss: 0.2678 - val_acc: 0.9483\n",
      "Epoch 20/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1947 - acc: 0.9473 - val_loss: 0.2354 - val_acc: 0.9484\n",
      "Epoch 21/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1943 - acc: 0.9474 - val_loss: 0.5090 - val_acc: 0.9381\n",
      "Epoch 22/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1938 - acc: 0.9473 - val_loss: 0.1899 - val_acc: 0.9485\n",
      "Epoch 23/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1925 - acc: 0.9473 - val_loss: 0.3621 - val_acc: 0.9486\n",
      "Epoch 24/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1923 - acc: 0.9474 - val_loss: 0.2404 - val_acc: 0.9484\n",
      "Epoch 25/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1917 - acc: 0.9476 - val_loss: 0.4101 - val_acc: 0.9483\n",
      "Epoch 26/200\n",
      "68792/68792 [==============================] - 67s 980us/step - loss: 0.1912 - acc: 0.9474 - val_loss: 0.6267 - val_acc: 0.9483\n",
      "Epoch 27/200\n",
      "68792/68792 [==============================] - 67s 972us/step - loss: 0.1911 - acc: 0.9475 - val_loss: 1.7154 - val_acc: 0.0520\n",
      "Epoch 28/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1907 - acc: 0.9475 - val_loss: 0.2041 - val_acc: 0.9485\n",
      "Epoch 29/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1907 - acc: 0.9474 - val_loss: 0.3347 - val_acc: 0.9318\n",
      "Epoch 30/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1900 - acc: 0.9477 - val_loss: 0.2286 - val_acc: 0.9469\n",
      "Epoch 31/200\n",
      "68792/68792 [==============================] - 68s 985us/step - loss: 0.1901 - acc: 0.9475 - val_loss: 0.8445 - val_acc: 0.2377\n",
      "Epoch 32/200\n",
      "68792/68792 [==============================] - 68s 988us/step - loss: 0.1900 - acc: 0.9476 - val_loss: 0.2371 - val_acc: 0.9476\n",
      "Epoch 33/200\n",
      "68792/68792 [==============================] - 68s 983us/step - loss: 0.1896 - acc: 0.9476 - val_loss: 0.1847 - val_acc: 0.9486\n",
      "Epoch 34/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1892 - acc: 0.9475 - val_loss: 0.2054 - val_acc: 0.9488\n",
      "Epoch 35/200\n",
      "68792/68792 [==============================] - 68s 993us/step - loss: 0.1895 - acc: 0.9475 - val_loss: 0.1866 - val_acc: 0.9488\n",
      "Epoch 36/200\n",
      "68792/68792 [==============================] - 69s 996us/step - loss: 0.1890 - acc: 0.9477 - val_loss: 0.3475 - val_acc: 0.9483\n",
      "Epoch 37/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1892 - acc: 0.9475 - val_loss: 0.7966 - val_acc: 0.3021\n",
      "Epoch 38/200\n",
      "68792/68792 [==============================] - 68s 988us/step - loss: 0.1888 - acc: 0.9475 - val_loss: 0.9745 - val_acc: 0.1628\n",
      "Epoch 39/200\n",
      "68792/68792 [==============================] - 67s 981us/step - loss: 0.1887 - acc: 0.9477 - val_loss: 0.2530 - val_acc: 0.9483\n",
      "Epoch 40/200\n",
      "68792/68792 [==============================] - 67s 981us/step - loss: 0.1883 - acc: 0.9476 - val_loss: 0.3156 - val_acc: 0.9483\n",
      "Epoch 41/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1890 - acc: 0.9475 - val_loss: 0.5203 - val_acc: 0.9483\n",
      "Epoch 42/200\n",
      "68792/68792 [==============================] - 68s 983us/step - loss: 0.1882 - acc: 0.9476 - val_loss: 1.2088 - val_acc: 0.0782\n",
      "Epoch 43/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1880 - acc: 0.9474 - val_loss: 0.2895 - val_acc: 0.9380\n",
      "Epoch 44/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1883 - acc: 0.9476 - val_loss: 0.2682 - val_acc: 0.9484\n",
      "Epoch 45/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1885 - acc: 0.9476 - val_loss: 0.1835 - val_acc: 0.9493\n",
      "Epoch 46/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1880 - acc: 0.9476 - val_loss: 0.1947 - val_acc: 0.9491\n",
      "Epoch 47/200\n",
      "68792/68792 [==============================] - 67s 980us/step - loss: 0.1881 - acc: 0.9477 - val_loss: 0.2943 - val_acc: 0.9485\n",
      "Epoch 48/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1879 - acc: 0.9477 - val_loss: 0.4031 - val_acc: 0.9483\n",
      "Epoch 49/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1878 - acc: 0.9478 - val_loss: 0.2804 - val_acc: 0.9486\n",
      "Epoch 50/200\n",
      "68792/68792 [==============================] - 66s 966us/step - loss: 0.1878 - acc: 0.9477 - val_loss: 0.3480 - val_acc: 0.9483\n",
      "Epoch 51/200\n",
      "68792/68792 [==============================] - 69s 1ms/step - loss: 0.1878 - acc: 0.9477 - val_loss: 0.2485 - val_acc: 0.9475\n",
      "Epoch 52/200\n",
      "68792/68792 [==============================] - 68s 993us/step - loss: 0.1876 - acc: 0.9478 - val_loss: 0.9000 - val_acc: 0.2393\n",
      "Epoch 53/200\n",
      "68792/68792 [==============================] - 68s 990us/step - loss: 0.1874 - acc: 0.9476 - val_loss: 1.5897 - val_acc: 0.0735\n",
      "Epoch 54/200\n",
      "68792/68792 [==============================] - 69s 1ms/step - loss: 0.1876 - acc: 0.9475 - val_loss: 0.3112 - val_acc: 0.9404\n",
      "Epoch 55/200\n",
      "68792/68792 [==============================] - 68s 985us/step - loss: 0.1878 - acc: 0.9477 - val_loss: 0.5253 - val_acc: 0.7961\n",
      "Epoch 56/200\n",
      "68792/68792 [==============================] - 68s 993us/step - loss: 0.1876 - acc: 0.9473 - val_loss: 0.2515 - val_acc: 0.9440\n",
      "Epoch 57/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1873 - acc: 0.9477 - val_loss: 0.1871 - val_acc: 0.9485\n",
      "Epoch 58/200\n",
      "68792/68792 [==============================] - 67s 981us/step - loss: 0.1873 - acc: 0.9476 - val_loss: 0.2150 - val_acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1871 - acc: 0.9476 - val_loss: 0.2364 - val_acc: 0.9484\n",
      "Epoch 60/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1871 - acc: 0.9476 - val_loss: 1.0558 - val_acc: 0.1271\n",
      "Epoch 61/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1873 - acc: 0.9477 - val_loss: 0.3103 - val_acc: 0.9483\n",
      "Epoch 62/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1867 - acc: 0.9477 - val_loss: 0.6398 - val_acc: 0.9483\n",
      "Epoch 63/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1869 - acc: 0.9474 - val_loss: 0.2812 - val_acc: 0.9483\n",
      "Epoch 64/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1869 - acc: 0.9477 - val_loss: 0.1895 - val_acc: 0.9489\n",
      "Epoch 65/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1869 - acc: 0.9476 - val_loss: 0.3254 - val_acc: 0.9310\n",
      "Epoch 66/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1867 - acc: 0.9475 - val_loss: 0.2901 - val_acc: 0.9483\n",
      "Epoch 67/200\n",
      "68792/68792 [==============================] - 67s 981us/step - loss: 0.1867 - acc: 0.9476 - val_loss: 0.4170 - val_acc: 0.8977\n",
      "Epoch 68/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1865 - acc: 0.9477 - val_loss: 0.1945 - val_acc: 0.9491\n",
      "Epoch 69/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1866 - acc: 0.9476 - val_loss: 0.2625 - val_acc: 0.9471\n",
      "Epoch 70/200\n",
      "68792/68792 [==============================] - 67s 972us/step - loss: 0.1864 - acc: 0.9476 - val_loss: 0.2946 - val_acc: 0.9363\n",
      "Epoch 71/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1866 - acc: 0.9477 - val_loss: 0.3294 - val_acc: 0.9385\n",
      "Epoch 72/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1866 - acc: 0.9475 - val_loss: 1.3901 - val_acc: 0.0895\n",
      "Epoch 73/200\n",
      "68792/68792 [==============================] - 66s 964us/step - loss: 0.1864 - acc: 0.9477 - val_loss: 0.2709 - val_acc: 0.9450\n",
      "Epoch 74/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1860 - acc: 0.9476 - val_loss: 0.1932 - val_acc: 0.9487\n",
      "Epoch 75/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1864 - acc: 0.9477 - val_loss: 0.3654 - val_acc: 0.9190\n",
      "Epoch 76/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1863 - acc: 0.9476 - val_loss: 0.2313 - val_acc: 0.9483\n",
      "Epoch 77/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1860 - acc: 0.9477 - val_loss: 0.3580 - val_acc: 0.9240\n",
      "Epoch 78/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1862 - acc: 0.9477 - val_loss: 0.5915 - val_acc: 0.9483\n",
      "Epoch 79/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1859 - acc: 0.9478 - val_loss: 0.2079 - val_acc: 0.9467\n",
      "Epoch 80/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1860 - acc: 0.9476 - val_loss: 0.3036 - val_acc: 0.9353\n",
      "Epoch 81/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1857 - acc: 0.9477 - val_loss: 0.6443 - val_acc: 0.6650\n",
      "Epoch 82/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1857 - acc: 0.9478 - val_loss: 0.1850 - val_acc: 0.9489\n",
      "Epoch 83/200\n",
      "68792/68792 [==============================] - 68s 990us/step - loss: 0.1857 - acc: 0.9477 - val_loss: 0.1956 - val_acc: 0.9491\n",
      "Epoch 84/200\n",
      "68792/68792 [==============================] - 66s 960us/step - loss: 0.1859 - acc: 0.9478 - val_loss: 1.1435 - val_acc: 0.1266\n",
      "Epoch 85/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1855 - acc: 0.9476 - val_loss: 0.2007 - val_acc: 0.9487\n",
      "Epoch 86/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1858 - acc: 0.9475 - val_loss: 0.8012 - val_acc: 0.9483\n",
      "Epoch 87/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1857 - acc: 0.9476 - val_loss: 0.2150 - val_acc: 0.9486\n",
      "Epoch 88/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1858 - acc: 0.9478 - val_loss: 1.6351 - val_acc: 0.0766\n",
      "Epoch 89/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1854 - acc: 0.9478 - val_loss: 0.2907 - val_acc: 0.9372\n",
      "Epoch 90/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1855 - acc: 0.9477 - val_loss: 0.8619 - val_acc: 0.3198\n",
      "Epoch 91/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1854 - acc: 0.9477 - val_loss: 0.3570 - val_acc: 0.8995\n",
      "Epoch 92/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1855 - acc: 0.9478 - val_loss: 0.7037 - val_acc: 0.5178\n",
      "Epoch 93/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1856 - acc: 0.9478 - val_loss: 0.1890 - val_acc: 0.9486\n",
      "Epoch 94/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1850 - acc: 0.9479 - val_loss: 0.2156 - val_acc: 0.9477\n",
      "Epoch 95/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1849 - acc: 0.9478 - val_loss: 0.3342 - val_acc: 0.9317\n",
      "Epoch 96/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1855 - acc: 0.9477 - val_loss: 0.1893 - val_acc: 0.9485\n",
      "Epoch 97/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1850 - acc: 0.9475 - val_loss: 1.0732 - val_acc: 0.1574\n",
      "Epoch 98/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1852 - acc: 0.9479 - val_loss: 0.1821 - val_acc: 0.9492\n",
      "Epoch 99/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1853 - acc: 0.9479 - val_loss: 0.2465 - val_acc: 0.9483\n",
      "Epoch 100/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1853 - acc: 0.9478 - val_loss: 0.1875 - val_acc: 0.9484\n",
      "Epoch 101/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1852 - acc: 0.9479 - val_loss: 0.6722 - val_acc: 0.6115\n",
      "Epoch 102/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1851 - acc: 0.9479 - val_loss: 0.7056 - val_acc: 0.4742\n",
      "Epoch 103/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1848 - acc: 0.9476 - val_loss: 0.2404 - val_acc: 0.9470\n",
      "Epoch 104/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1853 - acc: 0.9479 - val_loss: 0.2285 - val_acc: 0.9462\n",
      "Epoch 105/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1851 - acc: 0.9477 - val_loss: 0.1873 - val_acc: 0.9486\n",
      "Epoch 106/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1847 - acc: 0.9479 - val_loss: 0.7299 - val_acc: 0.4334\n",
      "Epoch 107/200\n",
      "68792/68792 [==============================] - 66s 963us/step - loss: 0.1852 - acc: 0.9478 - val_loss: 0.4751 - val_acc: 0.8504\n",
      "Epoch 108/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1851 - acc: 0.9478 - val_loss: 0.1964 - val_acc: 0.9471\n",
      "Epoch 109/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1851 - acc: 0.9477 - val_loss: 0.2869 - val_acc: 0.9399\n",
      "Epoch 110/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1845 - acc: 0.9475 - val_loss: 0.1832 - val_acc: 0.9491\n",
      "Epoch 111/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1848 - acc: 0.9479 - val_loss: 0.1976 - val_acc: 0.9474\n",
      "Epoch 112/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1847 - acc: 0.9477 - val_loss: 0.6282 - val_acc: 0.6607\n",
      "Epoch 113/200\n",
      "68792/68792 [==============================] - 67s 981us/step - loss: 0.1848 - acc: 0.9478 - val_loss: 0.1945 - val_acc: 0.9479\n",
      "Epoch 114/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1849 - acc: 0.9477 - val_loss: 0.1908 - val_acc: 0.9489\n",
      "Epoch 115/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1846 - acc: 0.9479 - val_loss: 0.3069 - val_acc: 0.9289\n",
      "Epoch 116/200\n",
      "68792/68792 [==============================] - 67s 972us/step - loss: 0.1845 - acc: 0.9477 - val_loss: 0.1882 - val_acc: 0.9486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1842 - acc: 0.9479 - val_loss: 0.6162 - val_acc: 0.6924\n",
      "Epoch 118/200\n",
      "68792/68792 [==============================] - 66s 967us/step - loss: 0.1841 - acc: 0.9478 - val_loss: 0.6639 - val_acc: 0.5891\n",
      "Epoch 119/200\n",
      "68792/68792 [==============================] - 67s 980us/step - loss: 0.1848 - acc: 0.9478 - val_loss: 0.2307 - val_acc: 0.9464\n",
      "Epoch 120/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1845 - acc: 0.9476 - val_loss: 0.7326 - val_acc: 0.4212\n",
      "Epoch 121/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1843 - acc: 0.9478 - val_loss: 0.8072 - val_acc: 0.3662\n",
      "Epoch 122/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1844 - acc: 0.9478 - val_loss: 0.2094 - val_acc: 0.9467\n",
      "Epoch 123/200\n",
      "68792/68792 [==============================] - 66s 966us/step - loss: 0.1844 - acc: 0.9477 - val_loss: 0.2454 - val_acc: 0.9441\n",
      "Epoch 124/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1847 - acc: 0.9477 - val_loss: 0.2260 - val_acc: 0.9443\n",
      "Epoch 125/200\n",
      "68792/68792 [==============================] - 66s 961us/step - loss: 0.1844 - acc: 0.9477 - val_loss: 0.7868 - val_acc: 0.4007\n",
      "Epoch 126/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1842 - acc: 0.9479 - val_loss: 0.1858 - val_acc: 0.9486\n",
      "Epoch 127/200\n",
      "68792/68792 [==============================] - 66s 966us/step - loss: 0.1841 - acc: 0.9481 - val_loss: 0.4370 - val_acc: 0.8855\n",
      "Epoch 128/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1840 - acc: 0.9478 - val_loss: 0.1861 - val_acc: 0.9489\n",
      "Epoch 129/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1840 - acc: 0.9479 - val_loss: 0.5795 - val_acc: 0.7408\n",
      "Epoch 130/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1840 - acc: 0.9477 - val_loss: 0.2259 - val_acc: 0.9477\n",
      "Epoch 131/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1839 - acc: 0.9478 - val_loss: 0.2437 - val_acc: 0.9486\n",
      "Epoch 132/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1840 - acc: 0.9480 - val_loss: 0.5616 - val_acc: 0.7821\n",
      "Epoch 133/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1839 - acc: 0.9479 - val_loss: 0.4728 - val_acc: 0.8560\n",
      "Epoch 134/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1840 - acc: 0.9479 - val_loss: 0.2414 - val_acc: 0.9475\n",
      "Epoch 135/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1840 - acc: 0.9480 - val_loss: 0.2001 - val_acc: 0.9467\n",
      "Epoch 136/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1842 - acc: 0.9478 - val_loss: 0.6706 - val_acc: 0.5748\n",
      "Epoch 137/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1839 - acc: 0.9479 - val_loss: 0.5310 - val_acc: 0.8141\n",
      "Epoch 138/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1842 - acc: 0.9478 - val_loss: 0.2636 - val_acc: 0.9428\n",
      "Epoch 139/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1842 - acc: 0.9477 - val_loss: 0.8297 - val_acc: 0.9483\n",
      "Epoch 140/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1838 - acc: 0.9479 - val_loss: 0.7265 - val_acc: 0.5384\n",
      "Epoch 141/200\n",
      "68792/68792 [==============================] - 67s 968us/step - loss: 0.1840 - acc: 0.9478 - val_loss: 0.1836 - val_acc: 0.9490\n",
      "Epoch 142/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1838 - acc: 0.9481 - val_loss: 0.3536 - val_acc: 0.9484\n",
      "Epoch 143/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1837 - acc: 0.9477 - val_loss: 0.3235 - val_acc: 0.9483\n",
      "Epoch 144/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1837 - acc: 0.9478 - val_loss: 0.2223 - val_acc: 0.9472\n",
      "Epoch 145/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1838 - acc: 0.9479 - val_loss: 0.5173 - val_acc: 0.8414\n",
      "Epoch 146/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1835 - acc: 0.9479 - val_loss: 0.4474 - val_acc: 0.8669\n",
      "Epoch 147/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1834 - acc: 0.9480 - val_loss: 0.3922 - val_acc: 0.8935\n",
      "Epoch 148/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1835 - acc: 0.9479 - val_loss: 0.2155 - val_acc: 0.9428\n",
      "Epoch 149/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1832 - acc: 0.9476 - val_loss: 0.3122 - val_acc: 0.9186\n",
      "Epoch 150/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1833 - acc: 0.9480 - val_loss: 0.3694 - val_acc: 0.9022\n",
      "Epoch 151/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1835 - acc: 0.9479 - val_loss: 0.2062 - val_acc: 0.9474\n",
      "Epoch 152/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1833 - acc: 0.9480 - val_loss: 0.1827 - val_acc: 0.9489\n",
      "Epoch 153/200\n",
      "68792/68792 [==============================] - 68s 986us/step - loss: 0.1833 - acc: 0.9479 - val_loss: 0.1933 - val_acc: 0.9490\n",
      "Epoch 154/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1833 - acc: 0.9477 - val_loss: 0.2575 - val_acc: 0.9483\n",
      "Epoch 155/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1834 - acc: 0.9478 - val_loss: 0.2138 - val_acc: 0.9485\n",
      "Epoch 156/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1832 - acc: 0.9479 - val_loss: 0.2969 - val_acc: 0.9422\n",
      "Epoch 157/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1832 - acc: 0.9478 - val_loss: 0.2698 - val_acc: 0.9483\n",
      "Epoch 158/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1831 - acc: 0.9477 - val_loss: 0.8598 - val_acc: 0.3337\n",
      "Epoch 159/200\n",
      "68792/68792 [==============================] - 66s 964us/step - loss: 0.1831 - acc: 0.9480 - val_loss: 0.2192 - val_acc: 0.9450\n",
      "Epoch 160/200\n",
      "68792/68792 [==============================] - 67s 980us/step - loss: 0.1834 - acc: 0.9480 - val_loss: 0.1880 - val_acc: 0.9489\n",
      "Epoch 161/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1830 - acc: 0.9480 - val_loss: 0.1817 - val_acc: 0.9491\n",
      "Epoch 162/200\n",
      "68792/68792 [==============================] - 68s 981us/step - loss: 0.1830 - acc: 0.9478 - val_loss: 0.1961 - val_acc: 0.9487\n",
      "Epoch 163/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1831 - acc: 0.9477 - val_loss: 0.5671 - val_acc: 0.7447\n",
      "Epoch 164/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1830 - acc: 0.9480 - val_loss: 0.2311 - val_acc: 0.9484\n",
      "Epoch 165/200\n",
      "68792/68792 [==============================] - 67s 976us/step - loss: 0.1832 - acc: 0.9478 - val_loss: 0.2044 - val_acc: 0.9483\n",
      "Epoch 166/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1831 - acc: 0.9478 - val_loss: 0.1890 - val_acc: 0.9489\n",
      "Epoch 167/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1828 - acc: 0.9477 - val_loss: 0.5376 - val_acc: 0.7696\n",
      "Epoch 168/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1828 - acc: 0.9480 - val_loss: 0.1830 - val_acc: 0.9486\n",
      "Epoch 169/200\n",
      "68792/68792 [==============================] - 67s 978us/step - loss: 0.1832 - acc: 0.9479 - val_loss: 0.1884 - val_acc: 0.9482\n",
      "Epoch 170/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1829 - acc: 0.9479 - val_loss: 0.3513 - val_acc: 0.9094\n",
      "Epoch 171/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1828 - acc: 0.9479 - val_loss: 0.2044 - val_acc: 0.9480\n",
      "Epoch 172/200\n",
      "68792/68792 [==============================] - 67s 971us/step - loss: 0.1825 - acc: 0.9479 - val_loss: 0.6150 - val_acc: 0.7056\n",
      "Epoch 173/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1827 - acc: 0.9479 - val_loss: 0.2826 - val_acc: 0.9381\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1825 - acc: 0.9480 - val_loss: 0.1867 - val_acc: 0.9479\n",
      "Epoch 175/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1826 - acc: 0.9479 - val_loss: 0.3629 - val_acc: 0.9133\n",
      "Epoch 176/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1830 - acc: 0.9478 - val_loss: 0.2185 - val_acc: 0.9462\n",
      "Epoch 177/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1823 - acc: 0.9480 - val_loss: 0.2663 - val_acc: 0.9372\n",
      "Epoch 178/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1824 - acc: 0.9482 - val_loss: 0.5106 - val_acc: 0.8120\n",
      "Epoch 179/200\n",
      "68792/68792 [==============================] - 67s 970us/step - loss: 0.1829 - acc: 0.9477 - val_loss: 0.5361 - val_acc: 0.8113\n",
      "Epoch 180/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1825 - acc: 0.9482 - val_loss: 0.2698 - val_acc: 0.9483\n",
      "Epoch 181/200\n",
      "68792/68792 [==============================] - 68s 982us/step - loss: 0.1823 - acc: 0.9479 - val_loss: 0.4990 - val_acc: 0.8292\n",
      "Epoch 182/200\n",
      "68792/68792 [==============================] - 67s 975us/step - loss: 0.1824 - acc: 0.9477 - val_loss: 0.4373 - val_acc: 0.8666\n",
      "Epoch 183/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1823 - acc: 0.9479 - val_loss: 0.5210 - val_acc: 0.7930\n",
      "Epoch 184/200\n",
      "68792/68792 [==============================] - 67s 967us/step - loss: 0.1824 - acc: 0.9479 - val_loss: 0.4447 - val_acc: 0.8575\n",
      "Epoch 185/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1824 - acc: 0.9478 - val_loss: 0.3096 - val_acc: 0.9483\n",
      "Epoch 186/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1824 - acc: 0.9481 - val_loss: 0.1935 - val_acc: 0.9489\n",
      "Epoch 187/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1827 - acc: 0.9480 - val_loss: 0.2767 - val_acc: 0.9483\n",
      "Epoch 188/200\n",
      "68792/68792 [==============================] - 67s 969us/step - loss: 0.1820 - acc: 0.9481 - val_loss: 0.2039 - val_acc: 0.9488\n",
      "Epoch 189/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1820 - acc: 0.9482 - val_loss: 0.1865 - val_acc: 0.9484\n",
      "Epoch 190/200\n",
      "68792/68792 [==============================] - 67s 973us/step - loss: 0.1822 - acc: 0.9481 - val_loss: 0.3077 - val_acc: 0.9288\n",
      "Epoch 191/200\n",
      "68792/68792 [==============================] - 66s 965us/step - loss: 0.1821 - acc: 0.9480 - val_loss: 0.2842 - val_acc: 0.9290\n",
      "Epoch 192/200\n",
      "68792/68792 [==============================] - 67s 972us/step - loss: 0.1821 - acc: 0.9480 - val_loss: 0.7424 - val_acc: 0.5382\n",
      "Epoch 193/200\n",
      "68792/68792 [==============================] - 66s 964us/step - loss: 0.1817 - acc: 0.9481 - val_loss: 0.1829 - val_acc: 0.9489\n",
      "Epoch 194/200\n",
      "68792/68792 [==============================] - 67s 972us/step - loss: 0.1820 - acc: 0.9480 - val_loss: 0.1837 - val_acc: 0.9488\n",
      "Epoch 195/200\n",
      "68792/68792 [==============================] - 66s 964us/step - loss: 0.1823 - acc: 0.9482 - val_loss: 0.1840 - val_acc: 0.9482\n",
      "Epoch 196/200\n",
      "68792/68792 [==============================] - 67s 977us/step - loss: 0.1817 - acc: 0.9482 - val_loss: 0.1844 - val_acc: 0.9489\n",
      "Epoch 197/200\n",
      "68792/68792 [==============================] - 66s 964us/step - loss: 0.1822 - acc: 0.9478 - val_loss: 0.3058 - val_acc: 0.9372\n",
      "Epoch 198/200\n",
      "68792/68792 [==============================] - 67s 974us/step - loss: 0.1821 - acc: 0.9480 - val_loss: 0.1841 - val_acc: 0.9482\n",
      "Epoch 199/200\n",
      "68792/68792 [==============================] - 67s 979us/step - loss: 0.1820 - acc: 0.9482 - val_loss: 0.1863 - val_acc: 0.9488\n",
      "Epoch 200/200\n",
      "68792/68792 [==============================] - 67s 972us/step - loss: 0.1813 - acc: 0.9481 - val_loss: 0.4977 - val_acc: 0.8260\n",
      "0.1813088998831882 0.8259675066967539\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import keras \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "      \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  \n",
    "nb_epochs = 200\n",
    "\n",
    "\n",
    "#flist = ['Adiac', 'Beef', 'CBF', 'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', \n",
    "#'DiatomSizeReduction', 'ECGFiveDays', 'FaceAll', 'FaceFour', 'FacesUCR', '50words', 'FISH', 'Gun_Point', 'Haptics', \n",
    "#'InlineSkate', 'ItalyPowerDemand', 'Lighting2', 'Lighting7', 'MALLAT', 'MedicalImages', 'MoteStrain', 'NonInvasiveFatalECG_Thorax1', \n",
    "#'NonInvasiveFatalECG_Thorax2', 'OliveOil', 'OSULeaf', 'SonyAIBORobotSurface', 'SonyAIBORobotSurfaceII', 'StarLightCurves', 'SwedishLeaf', 'Symbols', \n",
    "#'synthetic_control', 'Trace', 'TwoLeadECG', 'Two_Patterns', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y', 'uWaveGestureLibrary_Z', 'wafer', 'WordsSynonyms', 'yoga']\n",
    "\n",
    "# flist  = ['Adiac']\n",
    "# for each in flist:\n",
    "# fname = each\n",
    "\n",
    "# x_train, y_train = readucr(fname+'/'+fname+'_TRAIN')\n",
    "# x_test, y_test = readucr(fname+'/'+fname+'_TEST')\n",
    "nb_classes = len(np.unique(y_test))\n",
    "batch_size = min(X_train.shape[0]/10, 16)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "\n",
    "print(X_train.__class__.__name__)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean)/(X_train_std)\n",
    "\n",
    "X_test = (X_test - X_train_mean)/(X_train_std)\n",
    "X_train = X_train.reshape(X_train.shape + (1,1,))\n",
    "X_test = X_test.reshape(X_test.shape + (1,1,))\n",
    "\n",
    "x = keras.layers.Input(X_train.shape[1:])\n",
    "#    drop_out = Dropout(0.2)(x)\n",
    "conv1 = keras.layers.Conv2D(128, 8, 1, border_mode='same')(x)\n",
    "conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n",
    "conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv1)\n",
    "conv2 = keras.layers.Conv2D(256, 5, 1, border_mode='same')(conv1)\n",
    "conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n",
    "conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, 1, border_mode='same')(conv2)\n",
    "conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n",
    "conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "full = keras.layers.pooling.GlobalAveragePooling2D()(conv3)    \n",
    "out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "\n",
    "\n",
    "model = Model(input=x, output=out)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks = [reduce_lr])\n",
    "#Print the testing results which has the lowest training loss.\n",
    "log = pd.DataFrame(hist.history)\n",
    "print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
