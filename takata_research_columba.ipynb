{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/columba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>commitdate</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2006/7/8 9:06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954434</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21140</td>\n",
       "      <td>8343.008333</td>\n",
       "      <td>1188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2006/5/28 22:53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.693298</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.091503</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2006/5/4 11:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2402</td>\n",
       "      <td>713.716667</td>\n",
       "      <td>2396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>2005/9/27 11:09</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.894836</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>232.5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16307</td>\n",
       "      <td>5914.816667</td>\n",
       "      <td>15716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>2005/1/24 11:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>804.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1778</td>\n",
       "      <td>712.283333</td>\n",
       "      <td>1774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transactionid       commitdate  ns  nm  nf   entropy        la        ld  \\\n",
       "0              1    2006/7/8 9:06   1   2   2  0.954434  0.102564  0.102564   \n",
       "1              6  2006/5/28 22:53   1   3   3  0.693298  0.333333  0.091503   \n",
       "2             10   2006/5/4 11:48   1   1   1  0.000000  0.097345  0.044248   \n",
       "3             28  2005/9/27 11:09   1   3   4  0.894836  0.035484  0.034409   \n",
       "4             41  2005/1/24 11:41   1   1   1  0.000000  0.103234  0.004975   \n",
       "\n",
       "      lt  fix  ndev  pd  npt    exp         rexp   sexp  bug  \n",
       "0   39.0    0     1  53  1.0  21140  8343.008333   1188    0  \n",
       "1  102.0    0     6  24  1.0     70    70.000000     70    0  \n",
       "2  113.0    1     3  41  1.0   2402   713.716667   2396    0  \n",
       "3  232.5    0     8  97  1.0  16307  5914.816667  15716    0  \n",
       "4  804.0    0     4   8  1.0   1778   712.283333   1774    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:21: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:22: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# ~expとn~を削除\n",
    "df[\"xexp\"]  = ((df[\"exp\"] + df[\"rexp\"] + df[\"sexp\"])/3)\n",
    "df[\"nx\"] = ((df[\"ns\"] + df[\"nm\"] )/2)\n",
    "df = df.ix[:,[0,1,2,3,4,5,6,7,8,9,10, 11, 12, 13, 14, 15, 17, 18 ,16]]\n",
    "df\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "dropFeatures = [\"transactionid\",\"la\", \"exp\", \"rexp\", \"sexp\", \"commitdate\", \"ndev\", \"npt\", \"ns\", \"nm\"]\n",
    "X.drop(dropFeatures, axis =1, inplace=True)\n",
    "y = df.iloc[:, 18]\n",
    "\n",
    "from sklearn.model_selection import  train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:0.011385917663574219[sec]\n",
      "ロジスティック回帰の混合行列 [[881  38]\n",
      " [361  57]]\n",
      "ロジスティック回帰での正答率 0.7015706806282722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "lr = LogisticRegression()\n",
    "start = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "y_p_lr = lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_lr))\n",
    "print (\"ロジスティック回帰での正答率\", accuracy_score(y_test, y_p_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:0.7950539588928223[sec]\n",
      "SVMでの正答率 0.6955871353777113\n",
      "ロジスティック回帰の混合行列 [[896  23]\n",
      " [384  34]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "y_p_svm = svm.predict(X_test)\n",
    "# 正答率を算出\n",
    "print('SVMでの正答率', accuracy_score(y_test, y_p_svm))\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:0.001522064208984375[sec]\n",
      "knnの混合行列 [[845  74]\n",
      " [301 117]]\n",
      "knnでの正答率 0.7195213163799551\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "list_nn = []\n",
    "list_score = []\n",
    "for k in range(1, 5):\n",
    "    knc = KNeighborsClassifier(n_neighbors=k)\n",
    "    knc.fit(X_train, y_train)\n",
    "    Y_pred = knc.predict(X_test)\n",
    "    score = knc.score(X_test, y_test)\n",
    "    list_nn.append(k)\n",
    "    list_score.append(score)\n",
    "\n",
    "#プロット\n",
    "plt.ylim(0.1, 1.0)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(list_nn, list_score)\n",
    "\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors=2)\n",
    "start = time.time()\n",
    "knc.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "Y_pred = knc.predict(X_test)\n",
    "score = knc.score(X_test, y_test)\n",
    "print(\"knnの混合行列\", confusion_matrix(y_test, Y_pred))\n",
    "print (\"knnでの正答率\", accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.convolutional import Conv1D, UpSampling1D\n",
    "from keras.layers.pooling import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_dim=8, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-5.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# NN\n",
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def bug_model(activation=\"relu\", optimizer=\"adam\", out_dim=100):\n",
    "    dl_model = Sequential()\n",
    "    dl_model.add(Dense(out_dim, input_dim=8, init='uniform', activation=activation))\n",
    "    dl_model.add(Dense(out_dim, init='uniform', activation='relu'))\n",
    "    dl_model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    dl_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return dl_model\n",
    "    \n",
    "activation = [\"relu\", \"sigmoid\"]\n",
    "optimizer = [\"adam\", \"adagrad\"]\n",
    "out_dim = [100, 200]\n",
    "nb_epoch = [5, 10, 100]\n",
    "batch_size = [5, 10, 100]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dl_model.fit(X_train, y_train, nb_epoch=200, batch_size=10)\n",
    "dl_model = KerasClassifier(build_fn=bug_model, verbose=0)\n",
    "param_grid = dict(activation=activation, \n",
    "                  optimizer=optimizer, \n",
    "                  out_dim=out_dim, \n",
    "                  nb_epoch=nb_epoch, \n",
    "                  batch_size=batch_size)\n",
    "grid = GridSearchCV(estimator=dl_model, param_grid=param_grid)\n",
    "\n",
    "start = time.time()\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "\n",
    "print (grid_result.best_score_)\n",
    "print (grid_result.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scores = dl_model.evaluate(X_test, y_test)\n",
    "\n",
    "# print(\"DLでの正答率\",round(scores[1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3118 samples, validate on 1337 samples\n",
      "Epoch 1/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.6259 - acc: 0.6838 - val_loss: 0.6379 - val_acc: 0.6889\n",
      "Epoch 2/200\n",
      "3118/3118 [==============================] - 3s 958us/step - loss: 0.6119 - acc: 0.6969 - val_loss: 0.6501 - val_acc: 0.6866\n",
      "Epoch 3/200\n",
      "3118/3118 [==============================] - 3s 968us/step - loss: 0.6088 - acc: 0.6976 - val_loss: 0.6954 - val_acc: 0.5759\n",
      "Epoch 4/200\n",
      "3118/3118 [==============================] - 3s 949us/step - loss: 0.6075 - acc: 0.6966 - val_loss: 0.6227 - val_acc: 0.6881\n",
      "Epoch 5/200\n",
      "3118/3118 [==============================] - 3s 952us/step - loss: 0.6037 - acc: 0.7017 - val_loss: 0.6300 - val_acc: 0.6896\n",
      "Epoch 6/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.6073 - acc: 0.7011 - val_loss: 0.6231 - val_acc: 0.6896\n",
      "Epoch 7/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.6052 - acc: 0.7017 - val_loss: 0.6250 - val_acc: 0.6911\n",
      "Epoch 8/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.6016 - acc: 0.7056 - val_loss: 0.6326 - val_acc: 0.6956\n",
      "Epoch 9/200\n",
      "3118/3118 [==============================] - 3s 966us/step - loss: 0.6005 - acc: 0.7046 - val_loss: 0.6393 - val_acc: 0.6978\n",
      "Epoch 10/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5972 - acc: 0.7014 - val_loss: 0.6442 - val_acc: 0.6918\n",
      "Epoch 11/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.6012 - acc: 0.7030 - val_loss: 0.6071 - val_acc: 0.6971\n",
      "Epoch 12/200\n",
      "3118/3118 [==============================] - 5s 1ms/step - loss: 0.5971 - acc: 0.7065 - val_loss: 0.6127 - val_acc: 0.6993\n",
      "Epoch 13/200\n",
      "3118/3118 [==============================] - 5s 1ms/step - loss: 0.5934 - acc: 0.7027 - val_loss: 0.6230 - val_acc: 0.6933\n",
      "Epoch 14/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5988 - acc: 0.7033 - val_loss: 0.6269 - val_acc: 0.7001\n",
      "Epoch 15/200\n",
      "3118/3118 [==============================] - 5s 1ms/step - loss: 0.5950 - acc: 0.7040 - val_loss: 0.6135 - val_acc: 0.7016\n",
      "Epoch 16/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5925 - acc: 0.7097 - val_loss: 0.6346 - val_acc: 0.6956\n",
      "Epoch 17/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5950 - acc: 0.7037 - val_loss: 0.6033 - val_acc: 0.7001\n",
      "Epoch 18/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5895 - acc: 0.7126 - val_loss: 0.6144 - val_acc: 0.7038\n",
      "Epoch 19/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5943 - acc: 0.7110 - val_loss: 0.6019 - val_acc: 0.7001\n",
      "Epoch 20/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5917 - acc: 0.7043 - val_loss: 0.6227 - val_acc: 0.7046\n",
      "Epoch 21/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5895 - acc: 0.7120 - val_loss: 0.6527 - val_acc: 0.6904\n",
      "Epoch 22/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5895 - acc: 0.7152 - val_loss: 0.6235 - val_acc: 0.6926\n",
      "Epoch 23/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5927 - acc: 0.7062 - val_loss: 0.6154 - val_acc: 0.6971\n",
      "Epoch 24/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5884 - acc: 0.7123 - val_loss: 0.6332 - val_acc: 0.6672\n",
      "Epoch 25/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5887 - acc: 0.7104 - val_loss: 0.7540 - val_acc: 0.4862\n",
      "Epoch 26/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5906 - acc: 0.7107 - val_loss: 0.6105 - val_acc: 0.7053\n",
      "Epoch 27/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5872 - acc: 0.7091 - val_loss: 0.6525 - val_acc: 0.6335\n",
      "Epoch 28/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5885 - acc: 0.7097 - val_loss: 0.6345 - val_acc: 0.6986\n",
      "Epoch 29/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5861 - acc: 0.7146 - val_loss: 0.6398 - val_acc: 0.6993\n",
      "Epoch 30/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5882 - acc: 0.7081 - val_loss: 0.6099 - val_acc: 0.7023\n",
      "Epoch 31/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5849 - acc: 0.7094 - val_loss: 0.6030 - val_acc: 0.7053\n",
      "Epoch 32/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5857 - acc: 0.7149 - val_loss: 0.6799 - val_acc: 0.6118\n",
      "Epoch 33/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5834 - acc: 0.7146 - val_loss: 0.6383 - val_acc: 0.6986\n",
      "Epoch 34/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5839 - acc: 0.7126 - val_loss: 0.6101 - val_acc: 0.7053\n",
      "Epoch 35/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5841 - acc: 0.7085 - val_loss: 0.6752 - val_acc: 0.6971\n",
      "Epoch 36/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5827 - acc: 0.7110 - val_loss: 0.6004 - val_acc: 0.7016\n",
      "Epoch 37/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5796 - acc: 0.7139 - val_loss: 0.6515 - val_acc: 0.6425\n",
      "Epoch 38/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5842 - acc: 0.7149 - val_loss: 0.5985 - val_acc: 0.7038\n",
      "Epoch 39/200\n",
      "3118/3118 [==============================] - 3s 987us/step - loss: 0.5850 - acc: 0.7117 - val_loss: 0.6232 - val_acc: 0.6986\n",
      "Epoch 40/200\n",
      "3118/3118 [==============================] - 3s 989us/step - loss: 0.5796 - acc: 0.7139 - val_loss: 0.6199 - val_acc: 0.7008\n",
      "Epoch 41/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5810 - acc: 0.7136 - val_loss: 0.6402 - val_acc: 0.6963\n",
      "Epoch 42/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5810 - acc: 0.7158 - val_loss: 0.6219 - val_acc: 0.6911\n",
      "Epoch 43/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5837 - acc: 0.7162 - val_loss: 0.6345 - val_acc: 0.6963\n",
      "Epoch 44/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5819 - acc: 0.7136 - val_loss: 0.6104 - val_acc: 0.7046\n",
      "Epoch 45/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5821 - acc: 0.7123 - val_loss: 0.6087 - val_acc: 0.7038\n",
      "Epoch 46/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5797 - acc: 0.7178 - val_loss: 0.6172 - val_acc: 0.6978\n",
      "Epoch 47/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5810 - acc: 0.7130 - val_loss: 0.6223 - val_acc: 0.6911\n",
      "Epoch 48/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5840 - acc: 0.7165 - val_loss: 0.5997 - val_acc: 0.7038\n",
      "Epoch 49/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5796 - acc: 0.7174 - val_loss: 0.6163 - val_acc: 0.7016\n",
      "Epoch 50/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5809 - acc: 0.7158 - val_loss: 0.6237 - val_acc: 0.7008\n",
      "Epoch 51/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5806 - acc: 0.7152 - val_loss: 0.6400 - val_acc: 0.7001\n",
      "Epoch 52/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5805 - acc: 0.7162 - val_loss: 0.5989 - val_acc: 0.7091\n",
      "Epoch 53/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5807 - acc: 0.7120 - val_loss: 0.6019 - val_acc: 0.7083\n",
      "Epoch 54/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5797 - acc: 0.7165 - val_loss: 0.6013 - val_acc: 0.7053\n",
      "Epoch 55/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5790 - acc: 0.7191 - val_loss: 0.6536 - val_acc: 0.6545\n",
      "Epoch 56/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5763 - acc: 0.7229 - val_loss: 0.6241 - val_acc: 0.6836\n",
      "Epoch 57/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5786 - acc: 0.7162 - val_loss: 0.6071 - val_acc: 0.6956\n",
      "Epoch 58/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5765 - acc: 0.7184 - val_loss: 0.6077 - val_acc: 0.7068\n",
      "Epoch 59/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5823 - acc: 0.7200 - val_loss: 0.6039 - val_acc: 0.7053\n",
      "Epoch 60/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5778 - acc: 0.7210 - val_loss: 0.6874 - val_acc: 0.7008\n",
      "Epoch 61/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5775 - acc: 0.7235 - val_loss: 0.6049 - val_acc: 0.7091\n",
      "Epoch 62/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5774 - acc: 0.7207 - val_loss: 0.6158 - val_acc: 0.7016\n",
      "Epoch 63/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5792 - acc: 0.7171 - val_loss: 0.5999 - val_acc: 0.7023\n",
      "Epoch 64/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5776 - acc: 0.7149 - val_loss: 0.5970 - val_acc: 0.7046\n",
      "Epoch 65/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5753 - acc: 0.7229 - val_loss: 0.6237 - val_acc: 0.6814\n",
      "Epoch 66/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5767 - acc: 0.7149 - val_loss: 0.6049 - val_acc: 0.7001\n",
      "Epoch 67/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5755 - acc: 0.7152 - val_loss: 0.6062 - val_acc: 0.7105\n",
      "Epoch 68/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5777 - acc: 0.7174 - val_loss: 0.5967 - val_acc: 0.7046\n",
      "Epoch 69/200\n",
      "3118/3118 [==============================] - 3s 996us/step - loss: 0.5792 - acc: 0.7139 - val_loss: 0.6034 - val_acc: 0.7098\n",
      "Epoch 70/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5757 - acc: 0.7194 - val_loss: 0.5959 - val_acc: 0.7098\n",
      "Epoch 71/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5768 - acc: 0.7133 - val_loss: 0.6000 - val_acc: 0.7076\n",
      "Epoch 72/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5737 - acc: 0.7197 - val_loss: 0.6272 - val_acc: 0.6694\n",
      "Epoch 73/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5755 - acc: 0.7152 - val_loss: 0.5998 - val_acc: 0.7053\n",
      "Epoch 74/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5714 - acc: 0.7210 - val_loss: 0.6076 - val_acc: 0.7128\n",
      "Epoch 75/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5775 - acc: 0.7216 - val_loss: 0.5977 - val_acc: 0.7046\n",
      "Epoch 76/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5745 - acc: 0.7207 - val_loss: 0.5983 - val_acc: 0.7083\n",
      "Epoch 77/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5767 - acc: 0.7178 - val_loss: 0.5944 - val_acc: 0.7120\n",
      "Epoch 78/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5732 - acc: 0.7210 - val_loss: 0.5943 - val_acc: 0.7068\n",
      "Epoch 79/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5734 - acc: 0.7219 - val_loss: 0.6252 - val_acc: 0.6784\n",
      "Epoch 80/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5702 - acc: 0.7232 - val_loss: 0.6015 - val_acc: 0.7083\n",
      "Epoch 81/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5758 - acc: 0.7242 - val_loss: 0.6178 - val_acc: 0.6821\n",
      "Epoch 82/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5745 - acc: 0.7178 - val_loss: 0.5948 - val_acc: 0.7068\n",
      "Epoch 83/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5747 - acc: 0.7226 - val_loss: 0.6046 - val_acc: 0.7031\n",
      "Epoch 84/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5729 - acc: 0.7216 - val_loss: 0.6026 - val_acc: 0.7083\n",
      "Epoch 85/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5711 - acc: 0.7200 - val_loss: 0.6230 - val_acc: 0.7031\n",
      "Epoch 86/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5722 - acc: 0.7255 - val_loss: 0.6014 - val_acc: 0.7076\n",
      "Epoch 87/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5708 - acc: 0.7184 - val_loss: 0.6117 - val_acc: 0.6829\n",
      "Epoch 88/200\n",
      "3118/3118 [==============================] - 3s 961us/step - loss: 0.5720 - acc: 0.7171 - val_loss: 0.6076 - val_acc: 0.7038\n",
      "Epoch 89/200\n",
      "3118/3118 [==============================] - 3s 955us/step - loss: 0.5733 - acc: 0.7235 - val_loss: 0.6102 - val_acc: 0.7031\n",
      "Epoch 90/200\n",
      "3118/3118 [==============================] - 3s 964us/step - loss: 0.5730 - acc: 0.7200 - val_loss: 0.5966 - val_acc: 0.7091\n",
      "Epoch 91/200\n",
      "3118/3118 [==============================] - 3s 963us/step - loss: 0.5720 - acc: 0.7194 - val_loss: 0.5930 - val_acc: 0.7128\n",
      "Epoch 92/200\n",
      "3118/3118 [==============================] - 3s 972us/step - loss: 0.5731 - acc: 0.7203 - val_loss: 0.5964 - val_acc: 0.7076\n",
      "Epoch 93/200\n",
      "3118/3118 [==============================] - 3s 968us/step - loss: 0.5710 - acc: 0.7197 - val_loss: 0.6086 - val_acc: 0.7016\n",
      "Epoch 94/200\n",
      "3118/3118 [==============================] - 3s 968us/step - loss: 0.5708 - acc: 0.7232 - val_loss: 0.5988 - val_acc: 0.7091\n",
      "Epoch 95/200\n",
      "3118/3118 [==============================] - 3s 974us/step - loss: 0.5718 - acc: 0.7264 - val_loss: 0.6666 - val_acc: 0.7001\n",
      "Epoch 96/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5737 - acc: 0.7213 - val_loss: 0.6076 - val_acc: 0.7053\n",
      "Epoch 97/200\n",
      "3118/3118 [==============================] - 3s 983us/step - loss: 0.5719 - acc: 0.7197 - val_loss: 0.6485 - val_acc: 0.6978\n",
      "Epoch 98/200\n",
      "3118/3118 [==============================] - 3s 966us/step - loss: 0.5690 - acc: 0.7216 - val_loss: 0.6236 - val_acc: 0.6859\n",
      "Epoch 99/200\n",
      "3118/3118 [==============================] - 3s 960us/step - loss: 0.5706 - acc: 0.7280 - val_loss: 0.6075 - val_acc: 0.7135\n",
      "Epoch 100/200\n",
      "3118/3118 [==============================] - 3s 963us/step - loss: 0.5687 - acc: 0.7258 - val_loss: 0.5965 - val_acc: 0.7120\n",
      "Epoch 101/200\n",
      "3118/3118 [==============================] - 3s 972us/step - loss: 0.5683 - acc: 0.7219 - val_loss: 0.6123 - val_acc: 0.6918\n",
      "Epoch 102/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5692 - acc: 0.7248 - val_loss: 0.6001 - val_acc: 0.7105\n",
      "Epoch 103/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5666 - acc: 0.7248 - val_loss: 0.5988 - val_acc: 0.7120\n",
      "Epoch 104/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5702 - acc: 0.7245 - val_loss: 0.6067 - val_acc: 0.7031\n",
      "Epoch 105/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5710 - acc: 0.7213 - val_loss: 0.6388 - val_acc: 0.6664\n",
      "Epoch 106/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5700 - acc: 0.7171 - val_loss: 0.5965 - val_acc: 0.7135\n",
      "Epoch 107/200\n",
      "3118/3118 [==============================] - 3s 990us/step - loss: 0.5652 - acc: 0.7229 - val_loss: 0.6106 - val_acc: 0.6814\n",
      "Epoch 108/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5696 - acc: 0.7251 - val_loss: 0.6373 - val_acc: 0.6530\n",
      "Epoch 109/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5669 - acc: 0.7277 - val_loss: 0.6026 - val_acc: 0.7098\n",
      "Epoch 110/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5665 - acc: 0.7216 - val_loss: 0.6227 - val_acc: 0.7038\n",
      "Epoch 111/200\n",
      "3118/3118 [==============================] - 3s 976us/step - loss: 0.5676 - acc: 0.7261 - val_loss: 0.6040 - val_acc: 0.7076\n",
      "Epoch 112/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5650 - acc: 0.7306 - val_loss: 0.6078 - val_acc: 0.7031\n",
      "Epoch 113/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5689 - acc: 0.7248 - val_loss: 0.6393 - val_acc: 0.6597\n",
      "Epoch 114/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5664 - acc: 0.7261 - val_loss: 0.6027 - val_acc: 0.7105\n",
      "Epoch 115/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5633 - acc: 0.7284 - val_loss: 0.5970 - val_acc: 0.7128\n",
      "Epoch 116/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5669 - acc: 0.7232 - val_loss: 0.6081 - val_acc: 0.7023\n",
      "Epoch 117/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5685 - acc: 0.7210 - val_loss: 0.6006 - val_acc: 0.7113\n",
      "Epoch 118/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5649 - acc: 0.7303 - val_loss: 0.6109 - val_acc: 0.7076\n",
      "Epoch 119/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5671 - acc: 0.7290 - val_loss: 0.6165 - val_acc: 0.7038\n",
      "Epoch 120/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5689 - acc: 0.7226 - val_loss: 0.6456 - val_acc: 0.7031\n",
      "Epoch 121/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5675 - acc: 0.7248 - val_loss: 0.6313 - val_acc: 0.6776\n",
      "Epoch 122/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5651 - acc: 0.7271 - val_loss: 0.6023 - val_acc: 0.7068\n",
      "Epoch 123/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5627 - acc: 0.7245 - val_loss: 0.5941 - val_acc: 0.7053\n",
      "Epoch 124/200\n",
      "3118/3118 [==============================] - 3s 992us/step - loss: 0.5630 - acc: 0.7271 - val_loss: 0.6254 - val_acc: 0.6776\n",
      "Epoch 125/200\n",
      "3118/3118 [==============================] - 3s 994us/step - loss: 0.5620 - acc: 0.7261 - val_loss: 0.6036 - val_acc: 0.7128\n",
      "Epoch 126/200\n",
      "3118/3118 [==============================] - 3s 994us/step - loss: 0.5623 - acc: 0.7271 - val_loss: 0.5960 - val_acc: 0.7098\n",
      "Epoch 127/200\n",
      "3118/3118 [==============================] - 3s 998us/step - loss: 0.5618 - acc: 0.7261 - val_loss: 0.7094 - val_acc: 0.5714\n",
      "Epoch 128/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5587 - acc: 0.7290 - val_loss: 0.5927 - val_acc: 0.7120\n",
      "Epoch 129/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5636 - acc: 0.7290 - val_loss: 0.6727 - val_acc: 0.6148\n",
      "Epoch 130/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5601 - acc: 0.7341 - val_loss: 0.5980 - val_acc: 0.7120\n",
      "Epoch 131/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5595 - acc: 0.7303 - val_loss: 0.5966 - val_acc: 0.7120\n",
      "Epoch 132/200\n",
      "3118/3118 [==============================] - 3s 979us/step - loss: 0.5605 - acc: 0.7258 - val_loss: 0.6656 - val_acc: 0.6111\n",
      "Epoch 133/200\n",
      "3118/3118 [==============================] - 3s 964us/step - loss: 0.5591 - acc: 0.7284 - val_loss: 0.6049 - val_acc: 0.7113\n",
      "Epoch 134/200\n",
      "3118/3118 [==============================] - 3s 987us/step - loss: 0.5601 - acc: 0.7284 - val_loss: 0.5998 - val_acc: 0.7068\n",
      "Epoch 135/200\n",
      "3118/3118 [==============================] - 3s 996us/step - loss: 0.5614 - acc: 0.7239 - val_loss: 0.6188 - val_acc: 0.7091\n",
      "Epoch 136/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5592 - acc: 0.7306 - val_loss: 0.6546 - val_acc: 0.6358\n",
      "Epoch 137/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5599 - acc: 0.7328 - val_loss: 0.6066 - val_acc: 0.7091\n",
      "Epoch 138/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5620 - acc: 0.7274 - val_loss: 0.6148 - val_acc: 0.7068\n",
      "Epoch 139/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5660 - acc: 0.7290 - val_loss: 0.6262 - val_acc: 0.6761\n",
      "Epoch 140/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5586 - acc: 0.7351 - val_loss: 0.6192 - val_acc: 0.6926\n",
      "Epoch 141/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5583 - acc: 0.7309 - val_loss: 0.5991 - val_acc: 0.7120\n",
      "Epoch 142/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5612 - acc: 0.7306 - val_loss: 0.6180 - val_acc: 0.6956\n",
      "Epoch 143/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5621 - acc: 0.7280 - val_loss: 0.6816 - val_acc: 0.5969\n",
      "Epoch 144/200\n",
      "3118/3118 [==============================] - 3s 987us/step - loss: 0.5556 - acc: 0.7338 - val_loss: 0.5956 - val_acc: 0.7098\n",
      "Epoch 145/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5585 - acc: 0.7309 - val_loss: 0.7124 - val_acc: 0.5677\n",
      "Epoch 146/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5566 - acc: 0.7316 - val_loss: 0.6201 - val_acc: 0.7053\n",
      "Epoch 147/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5571 - acc: 0.7316 - val_loss: 0.5917 - val_acc: 0.7091\n",
      "Epoch 148/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5522 - acc: 0.7290 - val_loss: 0.7908 - val_acc: 0.4652\n",
      "Epoch 149/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5555 - acc: 0.7335 - val_loss: 0.6542 - val_acc: 0.7001\n",
      "Epoch 150/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5558 - acc: 0.7303 - val_loss: 0.6402 - val_acc: 0.6567\n",
      "Epoch 151/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5541 - acc: 0.7290 - val_loss: 0.6292 - val_acc: 0.6791\n",
      "Epoch 152/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5529 - acc: 0.7328 - val_loss: 0.5934 - val_acc: 0.7143\n",
      "Epoch 153/200\n",
      "3118/3118 [==============================] - 3s 996us/step - loss: 0.5532 - acc: 0.7316 - val_loss: 0.5948 - val_acc: 0.7113\n",
      "Epoch 154/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5517 - acc: 0.7348 - val_loss: 0.6392 - val_acc: 0.6567\n",
      "Epoch 155/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5506 - acc: 0.7341 - val_loss: 0.6581 - val_acc: 0.7031\n",
      "Epoch 156/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5566 - acc: 0.7284 - val_loss: 0.6183 - val_acc: 0.6904\n",
      "Epoch 157/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5515 - acc: 0.7389 - val_loss: 0.6394 - val_acc: 0.7008\n",
      "Epoch 158/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5530 - acc: 0.7341 - val_loss: 0.6975 - val_acc: 0.5879\n",
      "Epoch 159/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5530 - acc: 0.7351 - val_loss: 0.5976 - val_acc: 0.6978\n",
      "Epoch 160/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5548 - acc: 0.7325 - val_loss: 0.6914 - val_acc: 0.5924\n",
      "Epoch 161/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5480 - acc: 0.7348 - val_loss: 0.6421 - val_acc: 0.7053\n",
      "Epoch 162/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5500 - acc: 0.7354 - val_loss: 0.5979 - val_acc: 0.7105\n",
      "Epoch 163/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5533 - acc: 0.7309 - val_loss: 0.6480 - val_acc: 0.7046\n",
      "Epoch 164/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5500 - acc: 0.7332 - val_loss: 0.5999 - val_acc: 0.7135\n",
      "Epoch 165/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5504 - acc: 0.7316 - val_loss: 0.5957 - val_acc: 0.7180\n",
      "Epoch 166/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5490 - acc: 0.7402 - val_loss: 0.6205 - val_acc: 0.6971\n",
      "Epoch 167/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5557 - acc: 0.7322 - val_loss: 0.6100 - val_acc: 0.7046\n",
      "Epoch 168/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5480 - acc: 0.7380 - val_loss: 0.6471 - val_acc: 0.6642\n",
      "Epoch 169/200\n",
      "3118/3118 [==============================] - 3s 991us/step - loss: 0.5469 - acc: 0.7415 - val_loss: 0.5949 - val_acc: 0.7083\n",
      "Epoch 170/200\n",
      "3118/3118 [==============================] - 3s 984us/step - loss: 0.5472 - acc: 0.7344 - val_loss: 0.6896 - val_acc: 0.6028\n",
      "Epoch 171/200\n",
      "3118/3118 [==============================] - 3s 984us/step - loss: 0.5454 - acc: 0.7300 - val_loss: 0.6127 - val_acc: 0.7046\n",
      "Epoch 172/200\n",
      "3118/3118 [==============================] - 3s 994us/step - loss: 0.5513 - acc: 0.7354 - val_loss: 0.6612 - val_acc: 0.6387\n",
      "Epoch 173/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5479 - acc: 0.7328 - val_loss: 0.6288 - val_acc: 0.6911\n",
      "Epoch 174/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5468 - acc: 0.7393 - val_loss: 0.6325 - val_acc: 0.6724\n",
      "Epoch 175/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5457 - acc: 0.7364 - val_loss: 0.6379 - val_acc: 0.6926\n",
      "Epoch 176/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5489 - acc: 0.7393 - val_loss: 0.5990 - val_acc: 0.7076\n",
      "Epoch 177/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5456 - acc: 0.7354 - val_loss: 0.5984 - val_acc: 0.7068\n",
      "Epoch 178/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5435 - acc: 0.7399 - val_loss: 0.5957 - val_acc: 0.7031\n",
      "Epoch 179/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5445 - acc: 0.7348 - val_loss: 0.5951 - val_acc: 0.7135\n",
      "Epoch 180/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5437 - acc: 0.7396 - val_loss: 0.6191 - val_acc: 0.6896\n",
      "Epoch 181/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5439 - acc: 0.7296 - val_loss: 0.6743 - val_acc: 0.7053\n",
      "Epoch 182/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5456 - acc: 0.7335 - val_loss: 0.7659 - val_acc: 0.7001\n",
      "Epoch 183/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5420 - acc: 0.7351 - val_loss: 0.6044 - val_acc: 0.7023\n",
      "Epoch 184/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5435 - acc: 0.7360 - val_loss: 0.5955 - val_acc: 0.7023\n",
      "Epoch 185/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5421 - acc: 0.7351 - val_loss: 0.7139 - val_acc: 0.5894\n",
      "Epoch 186/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5435 - acc: 0.7421 - val_loss: 0.6399 - val_acc: 0.6821\n",
      "Epoch 187/200\n",
      "3118/3118 [==============================] - 3s 991us/step - loss: 0.5445 - acc: 0.7338 - val_loss: 0.6330 - val_acc: 0.7023\n",
      "Epoch 188/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5411 - acc: 0.7396 - val_loss: 0.6230 - val_acc: 0.7023\n",
      "Epoch 189/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5468 - acc: 0.7402 - val_loss: 0.6056 - val_acc: 0.6978\n",
      "Epoch 190/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5410 - acc: 0.7360 - val_loss: 0.5945 - val_acc: 0.7091\n",
      "Epoch 191/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5376 - acc: 0.7354 - val_loss: 0.7889 - val_acc: 0.4862\n",
      "Epoch 192/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5381 - acc: 0.7373 - val_loss: 0.6004 - val_acc: 0.7031\n",
      "Epoch 193/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5398 - acc: 0.7367 - val_loss: 0.6286 - val_acc: 0.6874\n",
      "Epoch 194/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5385 - acc: 0.7399 - val_loss: 0.6016 - val_acc: 0.7203\n",
      "Epoch 195/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5397 - acc: 0.7386 - val_loss: 0.6030 - val_acc: 0.7105\n",
      "Epoch 196/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5420 - acc: 0.7357 - val_loss: 0.6398 - val_acc: 0.7068\n",
      "Epoch 197/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5419 - acc: 0.7386 - val_loss: 0.7044 - val_acc: 0.5946\n",
      "Epoch 198/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5386 - acc: 0.7431 - val_loss: 0.6064 - val_acc: 0.7076\n",
      "Epoch 199/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5409 - acc: 0.7421 - val_loss: 0.6049 - val_acc: 0.7150\n",
      "Epoch 200/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5395 - acc: 0.7409 - val_loss: 0.7441 - val_acc: 0.5557\n",
      "0.5375734020151184 0.486163051619223\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "\n",
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import keras \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "      \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  \n",
    "nb_epochs = 200\n",
    "nb_classes = len(np.unique(y_test))\n",
    "batch_size = min(X_train.shape[0]/10, 16)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "\n",
    "print(X_train.__class__.__name__)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean)/(X_train_std)\n",
    "\n",
    "X_test = (X_test - X_train_mean)/(X_train_std)\n",
    "X_train = X_train.reshape(X_train.shape + (1,1,))\n",
    "X_test = X_test.reshape(X_test.shape + (1,1,))\n",
    "\n",
    "x = keras.layers.Input(X_train.shape[1:])\n",
    "#    drop_out = Dropout(0.2)(x)\n",
    "conv1 = keras.layers.Conv2D(128, 8, 1, border_mode='same')(x)\n",
    "conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n",
    "conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv1)\n",
    "conv2 = keras.layers.Conv2D(256, 5, 1, border_mode='same')(conv1)\n",
    "conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n",
    "conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, 1, border_mode='same')(conv2)\n",
    "conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n",
    "conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "full = keras.layers.pooling.GlobalAveragePooling2D()(conv3)    \n",
    "out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "\n",
    "\n",
    "model = Model(input=x, output=out)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "\n",
    "start = time.time()\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks = [reduce_lr])\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "#Print the testing results which has the lowest training loss.\n",
    "log = pd.DataFrame(hist.history)\n",
    "print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hatorikoudai/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/ipykernel/__main__.py:32: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_6_input to have shape (13,) but got array with shape (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-49c5af0bc680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# モデル訓練\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"elapsed_time:{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"[sec]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hatorikoudai/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hatorikoudai/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hatorikoudai/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_6_input to have shape (13,) but got array with shape (8,)"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def build_multilayer_perceptron():\n",
    "#     \"\"\"多層パーセプトロンモデルを構築\"\"\"\n",
    "    rnn_model = Sequential()\n",
    "    rnn_model.add(Dense(16, input_shape=(13, )))\n",
    "    rnn_model.add(Activation('relu'))\n",
    "    rnn_model.add(Dense(2))\n",
    "    rnn_model.add(Activation('softmax'))\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# モデル構築\n",
    "rnn_model = build_multilayer_perceptron()\n",
    "rnn_model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# モデル訓練\n",
    "start = time.time()\n",
    "rnn_model.fit(X_train, y_train, nb_epoch=200, batch_size=3, verbose=1)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "\n",
    "# モデル評価\n",
    "loss, accuracy = rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy = {:.3f}\".format(accuracy))\n",
    "\n",
    "print (\"RNNでの正答率\", round(accuracy,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
