{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/columba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>commitdate</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2006/7/8 9:06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954434</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21140</td>\n",
       "      <td>8343.008333</td>\n",
       "      <td>1188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2006/5/28 22:53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.693298</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.091503</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2006/5/4 11:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2402</td>\n",
       "      <td>713.716667</td>\n",
       "      <td>2396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>2005/9/27 11:09</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.894836</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>232.5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16307</td>\n",
       "      <td>5914.816667</td>\n",
       "      <td>15716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>2005/1/24 11:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>804.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1778</td>\n",
       "      <td>712.283333</td>\n",
       "      <td>1774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transactionid       commitdate  ns  nm  nf   entropy        la        ld  \\\n",
       "0              1    2006/7/8 9:06   1   2   2  0.954434  0.102564  0.102564   \n",
       "1              6  2006/5/28 22:53   1   3   3  0.693298  0.333333  0.091503   \n",
       "2             10   2006/5/4 11:48   1   1   1  0.000000  0.097345  0.044248   \n",
       "3             28  2005/9/27 11:09   1   3   4  0.894836  0.035484  0.034409   \n",
       "4             41  2005/1/24 11:41   1   1   1  0.000000  0.103234  0.004975   \n",
       "\n",
       "      lt  fix  ndev  pd  npt    exp         rexp   sexp  bug  \n",
       "0   39.0    0     1  53  1.0  21140  8343.008333   1188    0  \n",
       "1  102.0    0     6  24  1.0     70    70.000000     70    0  \n",
       "2  113.0    1     3  41  1.0   2402   713.716667   2396    0  \n",
       "3  232.5    0     8  97  1.0  16307  5914.816667  15716    0  \n",
       "4  804.0    0     4   8  1.0   1778   712.283333   1774    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X.drop(\"commitdate\", axis =1, inplace=True)\n",
    "y = df.iloc[:, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3118, 15)\n",
      "(1337, 15)\n",
      "(3118,)\n",
      "(1337,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ロジスティック回帰の混合行列 [[885  34]\n",
      " [354  64]]\n",
      "ロジスティック回帰での正答率 0.7097980553477936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_p_lr = lr.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_lr))\n",
    "\n",
    "print (\"ロジスティック回帰での正答率\", accuracy_score(y_test, y_p_lr))\n",
    "#ロジスティック回帰正答率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMでの正答率 0.7090501121914734\n",
      "ロジスティック回帰の混合行列 [[884  35]\n",
      " [354  64]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_p_svm = svm.predict(X_test)\n",
    "# 正答率を算出\n",
    "print('SVMでの正答率', accuracy_score(y_test, y_p_svm))\n",
    "print(\"ロジスティック回帰の混合行列\", confusion_matrix(y_test, y_p_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knnの混合行列 [[832  87]\n",
      " [340  78]]\n",
      "knnでの正答率 0.680628272251309\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "list_nn = []\n",
    "list_score = []\n",
    "for k in range(1, 5):\n",
    "    knc = KNeighborsClassifier(n_neighbors=k)\n",
    "    knc.fit(X_train, y_train)\n",
    "    Y_pred = knc.predict(X_test)\n",
    "    score = knc.score(X_test, y_test)\n",
    "#     print(\"[%d] score: {:.2f}\".format(score) % k)\n",
    "    list_nn.append(k)\n",
    "    list_score.append(score)\n",
    "\n",
    "#プロット\n",
    "plt.ylim(0.1, 1.0)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(list_nn, list_score)\n",
    "\n",
    "#k = 2の時が良い\n",
    "knc = KNeighborsClassifier(n_neighbors=2)\n",
    "knc.fit(X_train, y_train)\n",
    "Y_pred = knc.predict(X_test)\n",
    "score = knc.score(X_test, y_test)\n",
    "print(\"knnの混合行列\", confusion_matrix(y_test, Y_pred))\n",
    "print (\"knnでの正答率\", accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.convolutional import Conv1D, UpSampling1D\n",
    "from keras.layers.pooling import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Add, Input\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# lookback = 5\n",
    "\n",
    "# #データを4次元化\n",
    "# X_train = X_train.reshape((len(X_train),lookback,1,1))\n",
    "# X_val = X_val.reshape((len(X_val),lookback,1,1))\n",
    "# X_test = X_test.reshape((len(X_test),lookback,1,1))\n",
    "\n",
    "# #CNNの学習\n",
    "# input_ = Input(shape=(lookback, 1,1))#横の数、縦の数、RGB\n",
    "\n",
    "# c = Conv2D(8, (3, 1),padding='same',activation='relu')(input_)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = MaxPooling2D(pool_size=(2, 1))(c)\n",
    "\n",
    "# c = Flatten()(c)\n",
    "# c = Dense(30,activation='relu')(c)\n",
    "# c = Dropout(0.2)(c)\n",
    "# c = Dense(4, activation='softmax')(c)\n",
    "\n",
    "# model = Model(input_, c)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# hist = model.fit(X_train, Y_train, batch_size = 10, epochs=100, verbose=1, shuffle=True,\n",
    "#                  validation_data = (X_val,Y_val))\n",
    "# #結果描画\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_loss'],label=\"val_loss\")\n",
    "# plt.plot(hist.history['loss'],label=\"train_loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()               \n",
    "# plt.plot(hist.history['val_acc'],label=\"val_acc\")\n",
    "# plt.plot(hist.history['acc'],label=\"train_acc\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 1), padding=\"same\")`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/Users/kodaihatori/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3118 samples, validate on 1337 samples\n",
      "Epoch 1/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.6259 - acc: 0.6838 - val_loss: 0.6379 - val_acc: 0.6889\n",
      "Epoch 2/200\n",
      "3118/3118 [==============================] - 3s 958us/step - loss: 0.6119 - acc: 0.6969 - val_loss: 0.6501 - val_acc: 0.6866\n",
      "Epoch 3/200\n",
      "3118/3118 [==============================] - 3s 968us/step - loss: 0.6088 - acc: 0.6976 - val_loss: 0.6954 - val_acc: 0.5759\n",
      "Epoch 4/200\n",
      "3118/3118 [==============================] - 3s 949us/step - loss: 0.6075 - acc: 0.6966 - val_loss: 0.6227 - val_acc: 0.6881\n",
      "Epoch 5/200\n",
      "3118/3118 [==============================] - 3s 952us/step - loss: 0.6037 - acc: 0.7017 - val_loss: 0.6300 - val_acc: 0.6896\n",
      "Epoch 6/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.6073 - acc: 0.7011 - val_loss: 0.6231 - val_acc: 0.6896\n",
      "Epoch 7/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.6052 - acc: 0.7017 - val_loss: 0.6250 - val_acc: 0.6911\n",
      "Epoch 8/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.6016 - acc: 0.7056 - val_loss: 0.6326 - val_acc: 0.6956\n",
      "Epoch 9/200\n",
      "3118/3118 [==============================] - 3s 966us/step - loss: 0.6005 - acc: 0.7046 - val_loss: 0.6393 - val_acc: 0.6978\n",
      "Epoch 10/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5972 - acc: 0.7014 - val_loss: 0.6442 - val_acc: 0.6918\n",
      "Epoch 11/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.6012 - acc: 0.7030 - val_loss: 0.6071 - val_acc: 0.6971\n",
      "Epoch 12/200\n",
      "3118/3118 [==============================] - 5s 1ms/step - loss: 0.5971 - acc: 0.7065 - val_loss: 0.6127 - val_acc: 0.6993\n",
      "Epoch 13/200\n",
      "3118/3118 [==============================] - 5s 1ms/step - loss: 0.5934 - acc: 0.7027 - val_loss: 0.6230 - val_acc: 0.6933\n",
      "Epoch 14/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5988 - acc: 0.7033 - val_loss: 0.6269 - val_acc: 0.7001\n",
      "Epoch 15/200\n",
      "3118/3118 [==============================] - 5s 1ms/step - loss: 0.5950 - acc: 0.7040 - val_loss: 0.6135 - val_acc: 0.7016\n",
      "Epoch 16/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5925 - acc: 0.7097 - val_loss: 0.6346 - val_acc: 0.6956\n",
      "Epoch 17/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5950 - acc: 0.7037 - val_loss: 0.6033 - val_acc: 0.7001\n",
      "Epoch 18/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5895 - acc: 0.7126 - val_loss: 0.6144 - val_acc: 0.7038\n",
      "Epoch 19/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5943 - acc: 0.7110 - val_loss: 0.6019 - val_acc: 0.7001\n",
      "Epoch 20/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5917 - acc: 0.7043 - val_loss: 0.6227 - val_acc: 0.7046\n",
      "Epoch 21/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5895 - acc: 0.7120 - val_loss: 0.6527 - val_acc: 0.6904\n",
      "Epoch 22/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5895 - acc: 0.7152 - val_loss: 0.6235 - val_acc: 0.6926\n",
      "Epoch 23/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5927 - acc: 0.7062 - val_loss: 0.6154 - val_acc: 0.6971\n",
      "Epoch 24/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5884 - acc: 0.7123 - val_loss: 0.6332 - val_acc: 0.6672\n",
      "Epoch 25/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5887 - acc: 0.7104 - val_loss: 0.7540 - val_acc: 0.4862\n",
      "Epoch 26/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5906 - acc: 0.7107 - val_loss: 0.6105 - val_acc: 0.7053\n",
      "Epoch 27/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5872 - acc: 0.7091 - val_loss: 0.6525 - val_acc: 0.6335\n",
      "Epoch 28/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5885 - acc: 0.7097 - val_loss: 0.6345 - val_acc: 0.6986\n",
      "Epoch 29/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5861 - acc: 0.7146 - val_loss: 0.6398 - val_acc: 0.6993\n",
      "Epoch 30/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5882 - acc: 0.7081 - val_loss: 0.6099 - val_acc: 0.7023\n",
      "Epoch 31/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5849 - acc: 0.7094 - val_loss: 0.6030 - val_acc: 0.7053\n",
      "Epoch 32/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5857 - acc: 0.7149 - val_loss: 0.6799 - val_acc: 0.6118\n",
      "Epoch 33/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5834 - acc: 0.7146 - val_loss: 0.6383 - val_acc: 0.6986\n",
      "Epoch 34/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5839 - acc: 0.7126 - val_loss: 0.6101 - val_acc: 0.7053\n",
      "Epoch 35/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5841 - acc: 0.7085 - val_loss: 0.6752 - val_acc: 0.6971\n",
      "Epoch 36/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5827 - acc: 0.7110 - val_loss: 0.6004 - val_acc: 0.7016\n",
      "Epoch 37/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5796 - acc: 0.7139 - val_loss: 0.6515 - val_acc: 0.6425\n",
      "Epoch 38/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5842 - acc: 0.7149 - val_loss: 0.5985 - val_acc: 0.7038\n",
      "Epoch 39/200\n",
      "3118/3118 [==============================] - 3s 987us/step - loss: 0.5850 - acc: 0.7117 - val_loss: 0.6232 - val_acc: 0.6986\n",
      "Epoch 40/200\n",
      "3118/3118 [==============================] - 3s 989us/step - loss: 0.5796 - acc: 0.7139 - val_loss: 0.6199 - val_acc: 0.7008\n",
      "Epoch 41/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5810 - acc: 0.7136 - val_loss: 0.6402 - val_acc: 0.6963\n",
      "Epoch 42/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5810 - acc: 0.7158 - val_loss: 0.6219 - val_acc: 0.6911\n",
      "Epoch 43/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5837 - acc: 0.7162 - val_loss: 0.6345 - val_acc: 0.6963\n",
      "Epoch 44/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5819 - acc: 0.7136 - val_loss: 0.6104 - val_acc: 0.7046\n",
      "Epoch 45/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5821 - acc: 0.7123 - val_loss: 0.6087 - val_acc: 0.7038\n",
      "Epoch 46/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5797 - acc: 0.7178 - val_loss: 0.6172 - val_acc: 0.6978\n",
      "Epoch 47/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5810 - acc: 0.7130 - val_loss: 0.6223 - val_acc: 0.6911\n",
      "Epoch 48/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5840 - acc: 0.7165 - val_loss: 0.5997 - val_acc: 0.7038\n",
      "Epoch 49/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5796 - acc: 0.7174 - val_loss: 0.6163 - val_acc: 0.7016\n",
      "Epoch 50/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5809 - acc: 0.7158 - val_loss: 0.6237 - val_acc: 0.7008\n",
      "Epoch 51/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5806 - acc: 0.7152 - val_loss: 0.6400 - val_acc: 0.7001\n",
      "Epoch 52/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5805 - acc: 0.7162 - val_loss: 0.5989 - val_acc: 0.7091\n",
      "Epoch 53/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5807 - acc: 0.7120 - val_loss: 0.6019 - val_acc: 0.7083\n",
      "Epoch 54/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5797 - acc: 0.7165 - val_loss: 0.6013 - val_acc: 0.7053\n",
      "Epoch 55/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5790 - acc: 0.7191 - val_loss: 0.6536 - val_acc: 0.6545\n",
      "Epoch 56/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5763 - acc: 0.7229 - val_loss: 0.6241 - val_acc: 0.6836\n",
      "Epoch 57/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5786 - acc: 0.7162 - val_loss: 0.6071 - val_acc: 0.6956\n",
      "Epoch 58/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5765 - acc: 0.7184 - val_loss: 0.6077 - val_acc: 0.7068\n",
      "Epoch 59/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5823 - acc: 0.7200 - val_loss: 0.6039 - val_acc: 0.7053\n",
      "Epoch 60/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5778 - acc: 0.7210 - val_loss: 0.6874 - val_acc: 0.7008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5775 - acc: 0.7235 - val_loss: 0.6049 - val_acc: 0.7091\n",
      "Epoch 62/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5774 - acc: 0.7207 - val_loss: 0.6158 - val_acc: 0.7016\n",
      "Epoch 63/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5792 - acc: 0.7171 - val_loss: 0.5999 - val_acc: 0.7023\n",
      "Epoch 64/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5776 - acc: 0.7149 - val_loss: 0.5970 - val_acc: 0.7046\n",
      "Epoch 65/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5753 - acc: 0.7229 - val_loss: 0.6237 - val_acc: 0.6814\n",
      "Epoch 66/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5767 - acc: 0.7149 - val_loss: 0.6049 - val_acc: 0.7001\n",
      "Epoch 67/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5755 - acc: 0.7152 - val_loss: 0.6062 - val_acc: 0.7105\n",
      "Epoch 68/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5777 - acc: 0.7174 - val_loss: 0.5967 - val_acc: 0.7046\n",
      "Epoch 69/200\n",
      "3118/3118 [==============================] - 3s 996us/step - loss: 0.5792 - acc: 0.7139 - val_loss: 0.6034 - val_acc: 0.7098\n",
      "Epoch 70/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5757 - acc: 0.7194 - val_loss: 0.5959 - val_acc: 0.7098\n",
      "Epoch 71/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5768 - acc: 0.7133 - val_loss: 0.6000 - val_acc: 0.7076\n",
      "Epoch 72/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5737 - acc: 0.7197 - val_loss: 0.6272 - val_acc: 0.6694\n",
      "Epoch 73/200\n",
      "3118/3118 [==============================] - 3s 997us/step - loss: 0.5755 - acc: 0.7152 - val_loss: 0.5998 - val_acc: 0.7053\n",
      "Epoch 74/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5714 - acc: 0.7210 - val_loss: 0.6076 - val_acc: 0.7128\n",
      "Epoch 75/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5775 - acc: 0.7216 - val_loss: 0.5977 - val_acc: 0.7046\n",
      "Epoch 76/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5745 - acc: 0.7207 - val_loss: 0.5983 - val_acc: 0.7083\n",
      "Epoch 77/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5767 - acc: 0.7178 - val_loss: 0.5944 - val_acc: 0.7120\n",
      "Epoch 78/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5732 - acc: 0.7210 - val_loss: 0.5943 - val_acc: 0.7068\n",
      "Epoch 79/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5734 - acc: 0.7219 - val_loss: 0.6252 - val_acc: 0.6784\n",
      "Epoch 80/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5702 - acc: 0.7232 - val_loss: 0.6015 - val_acc: 0.7083\n",
      "Epoch 81/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5758 - acc: 0.7242 - val_loss: 0.6178 - val_acc: 0.6821\n",
      "Epoch 82/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5745 - acc: 0.7178 - val_loss: 0.5948 - val_acc: 0.7068\n",
      "Epoch 83/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5747 - acc: 0.7226 - val_loss: 0.6046 - val_acc: 0.7031\n",
      "Epoch 84/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5729 - acc: 0.7216 - val_loss: 0.6026 - val_acc: 0.7083\n",
      "Epoch 85/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5711 - acc: 0.7200 - val_loss: 0.6230 - val_acc: 0.7031\n",
      "Epoch 86/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5722 - acc: 0.7255 - val_loss: 0.6014 - val_acc: 0.7076\n",
      "Epoch 87/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5708 - acc: 0.7184 - val_loss: 0.6117 - val_acc: 0.6829\n",
      "Epoch 88/200\n",
      "3118/3118 [==============================] - 3s 961us/step - loss: 0.5720 - acc: 0.7171 - val_loss: 0.6076 - val_acc: 0.7038\n",
      "Epoch 89/200\n",
      "3118/3118 [==============================] - 3s 955us/step - loss: 0.5733 - acc: 0.7235 - val_loss: 0.6102 - val_acc: 0.7031\n",
      "Epoch 90/200\n",
      "3118/3118 [==============================] - 3s 964us/step - loss: 0.5730 - acc: 0.7200 - val_loss: 0.5966 - val_acc: 0.7091\n",
      "Epoch 91/200\n",
      "3118/3118 [==============================] - 3s 963us/step - loss: 0.5720 - acc: 0.7194 - val_loss: 0.5930 - val_acc: 0.7128\n",
      "Epoch 92/200\n",
      "3118/3118 [==============================] - 3s 972us/step - loss: 0.5731 - acc: 0.7203 - val_loss: 0.5964 - val_acc: 0.7076\n",
      "Epoch 93/200\n",
      "3118/3118 [==============================] - 3s 968us/step - loss: 0.5710 - acc: 0.7197 - val_loss: 0.6086 - val_acc: 0.7016\n",
      "Epoch 94/200\n",
      "3118/3118 [==============================] - 3s 968us/step - loss: 0.5708 - acc: 0.7232 - val_loss: 0.5988 - val_acc: 0.7091\n",
      "Epoch 95/200\n",
      "3118/3118 [==============================] - 3s 974us/step - loss: 0.5718 - acc: 0.7264 - val_loss: 0.6666 - val_acc: 0.7001\n",
      "Epoch 96/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5737 - acc: 0.7213 - val_loss: 0.6076 - val_acc: 0.7053\n",
      "Epoch 97/200\n",
      "3118/3118 [==============================] - 3s 983us/step - loss: 0.5719 - acc: 0.7197 - val_loss: 0.6485 - val_acc: 0.6978\n",
      "Epoch 98/200\n",
      "3118/3118 [==============================] - 3s 966us/step - loss: 0.5690 - acc: 0.7216 - val_loss: 0.6236 - val_acc: 0.6859\n",
      "Epoch 99/200\n",
      "3118/3118 [==============================] - 3s 960us/step - loss: 0.5706 - acc: 0.7280 - val_loss: 0.6075 - val_acc: 0.7135\n",
      "Epoch 100/200\n",
      "3118/3118 [==============================] - 3s 963us/step - loss: 0.5687 - acc: 0.7258 - val_loss: 0.5965 - val_acc: 0.7120\n",
      "Epoch 101/200\n",
      "3118/3118 [==============================] - 3s 972us/step - loss: 0.5683 - acc: 0.7219 - val_loss: 0.6123 - val_acc: 0.6918\n",
      "Epoch 102/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5692 - acc: 0.7248 - val_loss: 0.6001 - val_acc: 0.7105\n",
      "Epoch 103/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5666 - acc: 0.7248 - val_loss: 0.5988 - val_acc: 0.7120\n",
      "Epoch 104/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5702 - acc: 0.7245 - val_loss: 0.6067 - val_acc: 0.7031\n",
      "Epoch 105/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5710 - acc: 0.7213 - val_loss: 0.6388 - val_acc: 0.6664\n",
      "Epoch 106/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5700 - acc: 0.7171 - val_loss: 0.5965 - val_acc: 0.7135\n",
      "Epoch 107/200\n",
      "3118/3118 [==============================] - 3s 990us/step - loss: 0.5652 - acc: 0.7229 - val_loss: 0.6106 - val_acc: 0.6814\n",
      "Epoch 108/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5696 - acc: 0.7251 - val_loss: 0.6373 - val_acc: 0.6530\n",
      "Epoch 109/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5669 - acc: 0.7277 - val_loss: 0.6026 - val_acc: 0.7098\n",
      "Epoch 110/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5665 - acc: 0.7216 - val_loss: 0.6227 - val_acc: 0.7038\n",
      "Epoch 111/200\n",
      "3118/3118 [==============================] - 3s 976us/step - loss: 0.5676 - acc: 0.7261 - val_loss: 0.6040 - val_acc: 0.7076\n",
      "Epoch 112/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5650 - acc: 0.7306 - val_loss: 0.6078 - val_acc: 0.7031\n",
      "Epoch 113/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5689 - acc: 0.7248 - val_loss: 0.6393 - val_acc: 0.6597\n",
      "Epoch 114/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5664 - acc: 0.7261 - val_loss: 0.6027 - val_acc: 0.7105\n",
      "Epoch 115/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5633 - acc: 0.7284 - val_loss: 0.5970 - val_acc: 0.7128\n",
      "Epoch 116/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5669 - acc: 0.7232 - val_loss: 0.6081 - val_acc: 0.7023\n",
      "Epoch 117/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5685 - acc: 0.7210 - val_loss: 0.6006 - val_acc: 0.7113\n",
      "Epoch 118/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5649 - acc: 0.7303 - val_loss: 0.6109 - val_acc: 0.7076\n",
      "Epoch 119/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5671 - acc: 0.7290 - val_loss: 0.6165 - val_acc: 0.7038\n",
      "Epoch 120/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5689 - acc: 0.7226 - val_loss: 0.6456 - val_acc: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5675 - acc: 0.7248 - val_loss: 0.6313 - val_acc: 0.6776\n",
      "Epoch 122/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5651 - acc: 0.7271 - val_loss: 0.6023 - val_acc: 0.7068\n",
      "Epoch 123/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5627 - acc: 0.7245 - val_loss: 0.5941 - val_acc: 0.7053\n",
      "Epoch 124/200\n",
      "3118/3118 [==============================] - 3s 992us/step - loss: 0.5630 - acc: 0.7271 - val_loss: 0.6254 - val_acc: 0.6776\n",
      "Epoch 125/200\n",
      "3118/3118 [==============================] - 3s 994us/step - loss: 0.5620 - acc: 0.7261 - val_loss: 0.6036 - val_acc: 0.7128\n",
      "Epoch 126/200\n",
      "3118/3118 [==============================] - 3s 994us/step - loss: 0.5623 - acc: 0.7271 - val_loss: 0.5960 - val_acc: 0.7098\n",
      "Epoch 127/200\n",
      "3118/3118 [==============================] - 3s 998us/step - loss: 0.5618 - acc: 0.7261 - val_loss: 0.7094 - val_acc: 0.5714\n",
      "Epoch 128/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5587 - acc: 0.7290 - val_loss: 0.5927 - val_acc: 0.7120\n",
      "Epoch 129/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5636 - acc: 0.7290 - val_loss: 0.6727 - val_acc: 0.6148\n",
      "Epoch 130/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5601 - acc: 0.7341 - val_loss: 0.5980 - val_acc: 0.7120\n",
      "Epoch 131/200\n",
      "3118/3118 [==============================] - 3s 995us/step - loss: 0.5595 - acc: 0.7303 - val_loss: 0.5966 - val_acc: 0.7120\n",
      "Epoch 132/200\n",
      "3118/3118 [==============================] - 3s 979us/step - loss: 0.5605 - acc: 0.7258 - val_loss: 0.6656 - val_acc: 0.6111\n",
      "Epoch 133/200\n",
      "3118/3118 [==============================] - 3s 964us/step - loss: 0.5591 - acc: 0.7284 - val_loss: 0.6049 - val_acc: 0.7113\n",
      "Epoch 134/200\n",
      "3118/3118 [==============================] - 3s 987us/step - loss: 0.5601 - acc: 0.7284 - val_loss: 0.5998 - val_acc: 0.7068\n",
      "Epoch 135/200\n",
      "3118/3118 [==============================] - 3s 996us/step - loss: 0.5614 - acc: 0.7239 - val_loss: 0.6188 - val_acc: 0.7091\n",
      "Epoch 136/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5592 - acc: 0.7306 - val_loss: 0.6546 - val_acc: 0.6358\n",
      "Epoch 137/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5599 - acc: 0.7328 - val_loss: 0.6066 - val_acc: 0.7091\n",
      "Epoch 138/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5620 - acc: 0.7274 - val_loss: 0.6148 - val_acc: 0.7068\n",
      "Epoch 139/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5660 - acc: 0.7290 - val_loss: 0.6262 - val_acc: 0.6761\n",
      "Epoch 140/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5586 - acc: 0.7351 - val_loss: 0.6192 - val_acc: 0.6926\n",
      "Epoch 141/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5583 - acc: 0.7309 - val_loss: 0.5991 - val_acc: 0.7120\n",
      "Epoch 142/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5612 - acc: 0.7306 - val_loss: 0.6180 - val_acc: 0.6956\n",
      "Epoch 143/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5621 - acc: 0.7280 - val_loss: 0.6816 - val_acc: 0.5969\n",
      "Epoch 144/200\n",
      "3118/3118 [==============================] - 3s 987us/step - loss: 0.5556 - acc: 0.7338 - val_loss: 0.5956 - val_acc: 0.7098\n",
      "Epoch 145/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5585 - acc: 0.7309 - val_loss: 0.7124 - val_acc: 0.5677\n",
      "Epoch 146/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5566 - acc: 0.7316 - val_loss: 0.6201 - val_acc: 0.7053\n",
      "Epoch 147/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5571 - acc: 0.7316 - val_loss: 0.5917 - val_acc: 0.7091\n",
      "Epoch 148/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5522 - acc: 0.7290 - val_loss: 0.7908 - val_acc: 0.4652\n",
      "Epoch 149/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5555 - acc: 0.7335 - val_loss: 0.6542 - val_acc: 0.7001\n",
      "Epoch 150/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5558 - acc: 0.7303 - val_loss: 0.6402 - val_acc: 0.6567\n",
      "Epoch 151/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5541 - acc: 0.7290 - val_loss: 0.6292 - val_acc: 0.6791\n",
      "Epoch 152/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5529 - acc: 0.7328 - val_loss: 0.5934 - val_acc: 0.7143\n",
      "Epoch 153/200\n",
      "3118/3118 [==============================] - 3s 996us/step - loss: 0.5532 - acc: 0.7316 - val_loss: 0.5948 - val_acc: 0.7113\n",
      "Epoch 154/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5517 - acc: 0.7348 - val_loss: 0.6392 - val_acc: 0.6567\n",
      "Epoch 155/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5506 - acc: 0.7341 - val_loss: 0.6581 - val_acc: 0.7031\n",
      "Epoch 156/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5566 - acc: 0.7284 - val_loss: 0.6183 - val_acc: 0.6904\n",
      "Epoch 157/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5515 - acc: 0.7389 - val_loss: 0.6394 - val_acc: 0.7008\n",
      "Epoch 158/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5530 - acc: 0.7341 - val_loss: 0.6975 - val_acc: 0.5879\n",
      "Epoch 159/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5530 - acc: 0.7351 - val_loss: 0.5976 - val_acc: 0.6978\n",
      "Epoch 160/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5548 - acc: 0.7325 - val_loss: 0.6914 - val_acc: 0.5924\n",
      "Epoch 161/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5480 - acc: 0.7348 - val_loss: 0.6421 - val_acc: 0.7053\n",
      "Epoch 162/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5500 - acc: 0.7354 - val_loss: 0.5979 - val_acc: 0.7105\n",
      "Epoch 163/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5533 - acc: 0.7309 - val_loss: 0.6480 - val_acc: 0.7046\n",
      "Epoch 164/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5500 - acc: 0.7332 - val_loss: 0.5999 - val_acc: 0.7135\n",
      "Epoch 165/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5504 - acc: 0.7316 - val_loss: 0.5957 - val_acc: 0.7180\n",
      "Epoch 166/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5490 - acc: 0.7402 - val_loss: 0.6205 - val_acc: 0.6971\n",
      "Epoch 167/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5557 - acc: 0.7322 - val_loss: 0.6100 - val_acc: 0.7046\n",
      "Epoch 168/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5480 - acc: 0.7380 - val_loss: 0.6471 - val_acc: 0.6642\n",
      "Epoch 169/200\n",
      "3118/3118 [==============================] - 3s 991us/step - loss: 0.5469 - acc: 0.7415 - val_loss: 0.5949 - val_acc: 0.7083\n",
      "Epoch 170/200\n",
      "3118/3118 [==============================] - 3s 984us/step - loss: 0.5472 - acc: 0.7344 - val_loss: 0.6896 - val_acc: 0.6028\n",
      "Epoch 171/200\n",
      "3118/3118 [==============================] - 3s 984us/step - loss: 0.5454 - acc: 0.7300 - val_loss: 0.6127 - val_acc: 0.7046\n",
      "Epoch 172/200\n",
      "3118/3118 [==============================] - 3s 994us/step - loss: 0.5513 - acc: 0.7354 - val_loss: 0.6612 - val_acc: 0.6387\n",
      "Epoch 173/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5479 - acc: 0.7328 - val_loss: 0.6288 - val_acc: 0.6911\n",
      "Epoch 174/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5468 - acc: 0.7393 - val_loss: 0.6325 - val_acc: 0.6724\n",
      "Epoch 175/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5457 - acc: 0.7364 - val_loss: 0.6379 - val_acc: 0.6926\n",
      "Epoch 176/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5489 - acc: 0.7393 - val_loss: 0.5990 - val_acc: 0.7076\n",
      "Epoch 177/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5456 - acc: 0.7354 - val_loss: 0.5984 - val_acc: 0.7068\n",
      "Epoch 178/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5435 - acc: 0.7399 - val_loss: 0.5957 - val_acc: 0.7031\n",
      "Epoch 179/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5445 - acc: 0.7348 - val_loss: 0.5951 - val_acc: 0.7135\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5437 - acc: 0.7396 - val_loss: 0.6191 - val_acc: 0.6896\n",
      "Epoch 181/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5439 - acc: 0.7296 - val_loss: 0.6743 - val_acc: 0.7053\n",
      "Epoch 182/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5456 - acc: 0.7335 - val_loss: 0.7659 - val_acc: 0.7001\n",
      "Epoch 183/200\n",
      "3118/3118 [==============================] - 5s 2ms/step - loss: 0.5420 - acc: 0.7351 - val_loss: 0.6044 - val_acc: 0.7023\n",
      "Epoch 184/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5435 - acc: 0.7360 - val_loss: 0.5955 - val_acc: 0.7023\n",
      "Epoch 185/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5421 - acc: 0.7351 - val_loss: 0.7139 - val_acc: 0.5894\n",
      "Epoch 186/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5435 - acc: 0.7421 - val_loss: 0.6399 - val_acc: 0.6821\n",
      "Epoch 187/200\n",
      "3118/3118 [==============================] - 3s 991us/step - loss: 0.5445 - acc: 0.7338 - val_loss: 0.6330 - val_acc: 0.7023\n",
      "Epoch 188/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5411 - acc: 0.7396 - val_loss: 0.6230 - val_acc: 0.7023\n",
      "Epoch 189/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5468 - acc: 0.7402 - val_loss: 0.6056 - val_acc: 0.6978\n",
      "Epoch 190/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5410 - acc: 0.7360 - val_loss: 0.5945 - val_acc: 0.7091\n",
      "Epoch 191/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5376 - acc: 0.7354 - val_loss: 0.7889 - val_acc: 0.4862\n",
      "Epoch 192/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5381 - acc: 0.7373 - val_loss: 0.6004 - val_acc: 0.7031\n",
      "Epoch 193/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5398 - acc: 0.7367 - val_loss: 0.6286 - val_acc: 0.6874\n",
      "Epoch 194/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5385 - acc: 0.7399 - val_loss: 0.6016 - val_acc: 0.7203\n",
      "Epoch 195/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5397 - acc: 0.7386 - val_loss: 0.6030 - val_acc: 0.7105\n",
      "Epoch 196/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5420 - acc: 0.7357 - val_loss: 0.6398 - val_acc: 0.7068\n",
      "Epoch 197/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5419 - acc: 0.7386 - val_loss: 0.7044 - val_acc: 0.5946\n",
      "Epoch 198/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5386 - acc: 0.7431 - val_loss: 0.6064 - val_acc: 0.7076\n",
      "Epoch 199/200\n",
      "3118/3118 [==============================] - 4s 1ms/step - loss: 0.5409 - acc: 0.7421 - val_loss: 0.6049 - val_acc: 0.7150\n",
      "Epoch 200/200\n",
      "3118/3118 [==============================] - 3s 1ms/step - loss: 0.5395 - acc: 0.7409 - val_loss: 0.7441 - val_acc: 0.5557\n",
      "0.5375734020151184 0.486163051619223\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import keras \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "      \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  \n",
    "nb_epochs = 200\n",
    "\n",
    "\n",
    "#flist = ['Adiac', 'Beef', 'CBF', 'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', \n",
    "#'DiatomSizeReduction', 'ECGFiveDays', 'FaceAll', 'FaceFour', 'FacesUCR', '50words', 'FISH', 'Gun_Point', 'Haptics', \n",
    "#'InlineSkate', 'ItalyPowerDemand', 'Lighting2', 'Lighting7', 'MALLAT', 'MedicalImages', 'MoteStrain', 'NonInvasiveFatalECG_Thorax1', \n",
    "#'NonInvasiveFatalECG_Thorax2', 'OliveOil', 'OSULeaf', 'SonyAIBORobotSurface', 'SonyAIBORobotSurfaceII', 'StarLightCurves', 'SwedishLeaf', 'Symbols', \n",
    "#'synthetic_control', 'Trace', 'TwoLeadECG', 'Two_Patterns', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y', 'uWaveGestureLibrary_Z', 'wafer', 'WordsSynonyms', 'yoga']\n",
    "\n",
    "# flist  = ['Adiac']\n",
    "# for each in flist:\n",
    "# fname = each\n",
    "\n",
    "# x_train, y_train = readucr(fname+'/'+fname+'_TRAIN')\n",
    "# x_test, y_test = readucr(fname+'/'+fname+'_TEST')\n",
    "nb_classes = len(np.unique(y_test))\n",
    "batch_size = min(X_train.shape[0]/10, 16)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "\n",
    "print(X_train.__class__.__name__)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean)/(X_train_std)\n",
    "\n",
    "X_test = (X_test - X_train_mean)/(X_train_std)\n",
    "X_train = X_train.reshape(X_train.shape + (1,1,))\n",
    "X_test = X_test.reshape(X_test.shape + (1,1,))\n",
    "\n",
    "x = keras.layers.Input(X_train.shape[1:])\n",
    "#    drop_out = Dropout(0.2)(x)\n",
    "conv1 = keras.layers.Conv2D(128, 8, 1, border_mode='same')(x)\n",
    "conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n",
    "conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv1)\n",
    "conv2 = keras.layers.Conv2D(256, 5, 1, border_mode='same')(conv1)\n",
    "conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n",
    "conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, 3, 1, border_mode='same')(conv2)\n",
    "conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n",
    "conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "full = keras.layers.pooling.GlobalAveragePooling2D()(conv3)    \n",
    "out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "\n",
    "\n",
    "model = Model(input=x, output=out)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks = [reduce_lr])\n",
    "#Print the testing results which has the lowest training loss.\n",
    "log = pd.DataFrame(hist.history)\n",
    "print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
